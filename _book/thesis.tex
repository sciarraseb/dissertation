% This is the Reed College LaTeX thesis template. Most of the work
% for the document class was done by Sam Noble (SN), as well as this
% template. Later comments etc. by Ben Salzberg (BTS). Additional
% restructuring and APA support by Jess Youngberg (JY).
% Your comments and suggestions are more than welcome; please email
% them to cus@reed.edu
%
% See https://www.reed.edu/cis/help/LaTeX/index.html for help. There are a
% great bunch of help pages there, with notes on
% getting started, bibtex, etc. Go there and read it if you're not
% already familiar with LaTeX.
%
% Any line that starts with a percent symbol is a comment.
% They won't show up in the document, and are useful for notes
% to yourself and explaining commands.
% Commenting also removes a line from the document;
% very handy for troubleshooting problems. -BTS

%%
%% Preamble
\documentclass[
12pt, % The default document font size, options: 10pt, 11pt, 12pt
twoside,
english]{guelphthesis}
%----------------------------------------------------------------------------------------
% PACKAGES
%----------------------------------------------------------------------------------------

\usepackage{tocloft} %needed for table of contents, list of figures, list of tables, list of appendices
\usepackage{graphicx,latexsym}
\usepackage{amsmath}
\usepackage{amssymb,amsthm}

\usepackage{longtable,booktabs,setspace}
\usepackage{lmodern}
\usepackage{float}
\usepackage{etoolbox}
\floatplacement{figure}{H}
% Thanks, @Xyv
\usepackage{calc}
% End of CII addition
\usepackage{rotating}
\usepackage{tocbibind} %includes list of figures, list of tables, and table of contents in table of contents
\usepackage{indentfirst} %needed so that first paragraph after each section titles has indent
\usepackage{lineno} %allows option for line numbering
\usepackage{draftwatermark} %for draft watermark
\SetWatermarkText{} %ensures draft is not printed when draft:false
%\usepackage[backend=biber]{biblatex}


% Syntax highlighting #22



% To pass between YAML and LaTeX the dollar signs are added by CII
\title{Is Timing Everything? The Effects of Measurement Timing on the Performance of Nonlinear Longitudinal Models}
\author{Sebastian L.V. Sciarra}
\year{2023}
\date{February, 2023}
\advisor{David Stanley}
\institution{University of Guelph}
\degree{Doctorate of Philosophy}



\field{Industrial-Organizational Psychology}
\department{Psychology}



  \let\cleardoublepage\clearpage

% From {rticles}
%


\urlstyle{rm}

%----------------------------------------------------------------------------------------
% CUSTOM COMMANDS
%----------------------------------------------------------------------------------------
%numbers lines before equations
%taken from https://tex.stackexchange.com/questions/43648/why-doesnt-lineno-number-a-paragraph-when-it-is-followed-by-an-align-equation
\newcommand*\patchAmsMathEnvironmentForLineno[1]{%
  \expandafter\let\csname old#1\expandafter\endcsname\csname #1\endcsname
  \expandafter\let\csname oldend#1\expandafter\endcsname\csname end#1\endcsname
  \renewenvironment{#1}%
     {\linenomath\csname old#1\endcsname}%
     {\csname oldend#1\endcsname\endlinenomath}}%
\newcommand*\patchBothAmsMathEnvironmentsForLineno[1]{%
  \patchAmsMathEnvironmentForLineno{#1}%
  \patchAmsMathEnvironmentForLineno{#1*}}%
\AtBeginDocument{%
\patchBothAmsMathEnvironmentsForLineno{equation}%
\patchBothAmsMathEnvironmentsForLineno{align}%
\patchBothAmsMathEnvironmentsForLineno{flalign}%
\patchBothAmsMathEnvironmentsForLineno{alignat}%
\patchBothAmsMathEnvironmentsForLineno{gather}%
\patchBothAmsMathEnvironmentsForLineno{multline}%
}


%nest all the \frontmatter functions in \oldfrontmatter, which allows us to redefine \frontmatter as everything it was with one modification to the
%draft watermark
\let\oldfrontmatter\frontmatter
%set page numbering to bottom center for \frontmatter
\fancypagestyle{frontmatter}{%
 \fancyhf{}% clear all header and footer fields
  \renewcommand{\headrulewidth}{0pt}
  \fancyhead[R]{\roman{page}}% Roman page number in footer centre

  }

\renewcommand{\frontmatter}{
  \oldfrontmatter
  
   %set page number font to Arial if ArialFont: false in YAML header
  
   \pagestyle{frontmatter} % add this to center page numbers
}

%set page numbering to bottom center for \mainmatter
\fancypagestyle{mainmatter}{%
 \fancyhf{}% clear all header and footer fields
  \renewcommand{\headrulewidth}{0pt}
  \fancyfoot[C]{\arabic{page}}% Roman page number in footer centre

   \hypersetup{pdfpagemode={UseOutlines},
    bookmarksopen=true,
    hypertexnames=true,
    colorlinks = true,
    allcolors = blue,
    %linkcolor = blue,
    %urlcolor= blue,
    %anchorcolor = blue,
    pdfstartview={FitV},
    breaklinks=true,
    hyperindex = true,
    backref=page}

  \cleardoublepage

  


}

%nest all the \mainmatter functions in \oldmainmatter, which allows us to redefine \mainmatter as everything it was with one modification to the
%page numbering format
\newcommand{\setMainMatterLinespacing}{
 \setstretch{2} %default line spacing

  %change line spacing if specified in YAML header
        \setstretch{2}
  }

\let\oldmainmatter\mainmatter
\renewcommand{\mainmatter}{
  \oldmainmatter

  %change line spacing if specified in YAML header
  \setMainMatterLinespacing

      \linenumbers
  
  \pagestyle{mainmatter} % add this to center page numbers

}

%code below is important for linespacing to remain unaffected when kableExtra::landscape() is used andthe margin is specifically defined. Otherwise,
%linespacing for entire document goes to singlespacing for the text that follows the table.
\let\oldRestoreGeometry\restoregeometry
\renewcommand{\restoregeometry}{
  \oldRestoreGeometry

  %change line spacing if specified in YAML header
  \setMainMatterLinespacing
}

%change footnote and page number font to arial if desired

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS, LIST OF FIGURES, & LIST OF TABLES
%----------------------------------------------------------------------------------------
%TABLE OF CONTENTS
\setlength{\cftbeforetoctitleskip}{0cm} %remove vertical space above table of contents

%two lines below ensure centered title for toc
%needed so that table of contents entry is not indented
\renewcommand{\contentsname}{Table of Contents} %change title for toc
\renewcommand{\cfttoctitlefont}{\hfill\fontsize{14}{14}\selectfont\bfseries\MakeUppercase}
\renewcommand{\cftaftertoctitle}{\hfill\hfill} %sometimes another \hfill is needed; depends on some setting in abovce code

%fonts for all entry level titles
\renewcommand\cftchapfont{\mdseries} %eliminate bolded chapter titles in toc
\renewcommand\cftsecfont{\mdseries} %eliminate bolded chapter titles in toc
\renewcommand\cftsubsecfont{\mdseries} %eliminate bolded chapter titles in toc
\renewcommand\cftsubsubsecfont{\mdseries} %eliminate bolded chapter titles in toc
\renewcommand\cftparafont{\mdseries} %eliminate bolded chapter titles in toc
\renewcommand\cftsubparafont{\mdseries} %eliminate bolded chapter titles in toc

%fonts for all entry level page numbers
\renewcommand{\cftchappagefont}{\mdseries} %remove bolding of page numbers for chapter headers in toc
\renewcommand\cftsecpagefont{\mdseries} %eliminate bolded chapter titles in toc
\renewcommand\cftsubsecpagefont{\mdseries} %eliminate bolded chapter titles in toc
\renewcommand\cftsubsubsecpagefont{\mdseries} %eliminate bolded chapter titles in toc
\renewcommand\cftparapagefont{\mdseries} %eliminate bolded chapter titles in toc
\renewcommand\cftsubparapagefont{\mdseries} %eliminate bolded chapter titles in toc

\renewcommand{\cftchapleader}{\cftdotfill{0.1}} %remove chapter bolding + modif dot spacing
\renewcommand{\cftdotsep}{0.1} %make dots in toc closer together

%spacing between toc items (should be all equal)
\setlength{\cftbeforechapskip}{0cm} %removes spacing before each chapter element
\renewcommand{\cftchapafterpnum}{\vskip6pt}
\renewcommand{\cftsecafterpnum}{\vskip6pt}
\renewcommand{\cftsubsecafterpnum}{\vskip6pt}
\renewcommand{\cftsubsubsecafterpnum}{\vskip6pt}
\renewcommand{\cftparaafterpnum}{\vskip6pt}
\renewcommand{\cftsubparaafterpnum}{\vskip6pt}

%remove header that appears in table of contents after first page
\renewcommand{\cftmarktoc}{}

%commands need to be redefined so that leading dots go all the way to the page numbers for all header levels (chap, sec, subsec, subsubsec, para, subpara
%%%general framework for commands below: cftXfillnum sets the format for the leading dots (\cftchapleader) and the page number (\cftchappagefont) such that leading dots proceed all the way to the page number with no spaces between dots and page number (\nobreak) at which wpoint paragraph mode ends (\par) and vertical spacing (defined  above) after item entry is inserted
%chapter (level 0)
\renewcommand{\cftchapfillnum}[1]{%
  {\cftchapleader}\nobreak
  {\cftchappagefont #1}\par\cftchapafterpnum
}

%sec (level 1)
\renewcommand{\cftsecfillnum}[1]{%
  {\cftsecleader}\nobreak
  {\cftsecpagefont #1}\par\cftsecafterpnum
}

%subsec (level 2)
\renewcommand{\cftsubsecfillnum}[1]{%
  {\cftsubsecleader}\nobreak
  {\cftsubsecpagefont #1}\par\cftsubsecafterpnum
}

%subsubsec (level 3)
\renewcommand{\cftsubsubsecfillnum}[1]{%
  {\cftsubsubsecleader}\nobreak
  {\cftsubsubsecpagefont #1}\par\cftsubsubsecafterpnum
}

%para (level 4)
\renewcommand{\cftparafillnum}[1]{%
  {\cftparaleader}\nobreak
  {\cftparapagefont #1}\par\cftparaafterpnum
}

%subpara (level 5)
\renewcommand{\cftsubparafillnum}[1]{%
  {\cftsubparaleader}\nobreak
  {\cftsubparapagefont #1}\par\cftsubparaafterpnum
}

%LIST OF TABLES
\renewcommand{\cfttabfont}{\mdseries} %set font for entries in lot
\renewcommand{\cfttabpagefont}{\mdseries} %set front for page numbers

\setlength{\cftbeforelottitleskip}{0cm} %remove vertical space above table of contents
\setlength{\cftafterlottitleskip}{0.5cm} %space between title for list of tables and list entries
%two lines below ensure centered title for toc
%needed so that table of contents entry is not indented
\renewcommand{\cftlottitlefont}{\hfill\fontsize{14}{14}\selectfont\bfseries\MakeUppercase}
\renewcommand{\cftafterlottitle}{\hfill} %sometimes another \hfill is needed; depends on some setting in abovce code

%commands need to be redefined so that leading dots go all the way to the page numbers for tables
%%%general framework for command below: cftfigfillnum sets the format for the leading dots (\cftfigleader) and the page number (\cftfigpagefont) such that leading dots proceed all the way to the page number with no spaces between dots and page number (\nobreak) at which point paragraph mode ends (\par) and vertical spacing (defined  below) after item entry is inserted
\setlength{\cftbeforetabskip}{0cm} %removes spacing before each chapter element
\renewcommand{\cfttabafterpnum}{\vskip6pt}

\renewcommand{\cfttabfillnum}[1]{%
  {\cfttableader}\nobreak
  {\cfttabpagefont #1}\par\cfttabafterpnum
}

%remove header that appears in list of tables after first page
\renewcommand{\cftmarklot}{}

%LIST OF FIGURES
\renewcommand{\cftfigfont}{\mdseries} %set font for entries in lof
\renewcommand{\cftfigpagefont}{\mdseries} %set front for page numbers

\setlength{\cftbeforeloftitleskip}{0cm} %remove vertical space above table of contents
\setlength{\cftafterloftitleskip}{0.5cm} %space between title for list of figures and list entries

%two lines below ensure centered title for toc
%needed so that table of contents entry is not indented
\renewcommand{\cftloftitlefont}{\hfill\fontsize{14}{14}\selectfont\bfseries\MakeUppercase}
\renewcommand{\cftafterloftitle}{\hfill} %sometimes another \hfill is needed; depends on some setting in abovce code

%commands need to be redefined so that leading dots go all the way to the page numbers for figures
%%%general framework for command below: cftfigfillnum sets the format for the leading dots (\cftfigleader) and the page number (\cftfigpagefont) such that leading dots proceed all the way to the page number with no spaces between dots and page number (\nobreak) at which wpoint paragraph mode ends (\par) and vertical spacing (defined  below) after item entry is inserted
\setlength{\cftbeforefigskip}{0cm} %removes spacing before each chapter element
\renewcommand{\cftfigafterpnum}{\vskip6pt}

\renewcommand{\cftfigfillnum}[1]{%
  {\cftfigleader}\nobreak
  {\cftfigpagefont #1}\par\cftfigafterpnum
}

%remove header that appears in list of figures after first page
\renewcommand{\cftmarklof}{}

%----------------------------------------------------------------------------------------
% LIST OF APPENDICES
%----------------------------------------------------------------------------------------
\newcommand{\listappname}{List of Appendices}
\newlistof[chapter]{app}{loa}{\listappname} %creates a new appendix counter that will be reset at the start of each \chapter

\setcounter{loadepth}{5} %loa will  go to depth of level 5
\setlength{\cftbeforeloatitleskip}{0cm} %remove vertical space above loa
\setlength{\cftafterloatitleskip}{0.5cm} %space between title for loa and list entries
\renewcommand{\cftmarkloa}{} %remove header titles

%two lines below ensure centered title for loa
%needed so that table of contents entry is not indented
\renewcommand{\cftloatitlefont}{\hfill\fontsize{14}{14}\selectfont\bfseries\MakeUppercase}
\renewcommand{\cftafterloatitle}{\hfill\hfill} %sometimes another \hfill is needed; depends on some setting in above code


%APPENDIX (level 0)
\renewcommand{\theapp}{\Alph{app}} %sets alphabetic counter for appendix
\renewcommand{\cftappfont}{\mdseries} %set font for level 0 entry in loa
\renewcommand{\cftapppagefont}{\mdseries} %set front for page numbers

\renewcommand{\cftapppresnum}{Appendix\space}
\renewcommand{\cftappaftersnum}{:\space}
\settowidth{\cftappnumwidth}{\cftapppresnum\theapp\cftappaftersnum\space}

\setlength{\cftbeforeappskip}{0cm} %removes vertical spacing before each chapter element
\renewcommand{\cftappafterpnum}{\vskip6pt}

%updates appendix counter, modifies chapter title such so that it is Appendix _letter_: #1
\newcommand{\app}[1]{%
  \refstepcounter{app}\pdfbookmark[-1]{\cftapppresnum\theapp\cftappaftersnum#1}{#1\theapp}%
  \chapter*{\fontsize{16}{16}\selectfont\bfseries\cftapppresnum\theapp\cftappaftersnum #1} %formats entry in document
  \addcontentsline{loa}{app}{{\cftapppresnum\theapp\cftappaftersnum}#1}%
  \par
}

% figure and table counting in appendix
\usepackage{chngcntr}


%leading dots for appendix (end immediately before page number)
\renewcommand{\cftappfillnum}[1]{%
 {\cftappleader}\nobreak{\cftapppagefont #1}\par\cftappafterpnum
}

%SECAPPENDIX (level 1; format A.1 : title)
\newlistentry[app]{secapp}{loa}{1}
\renewcommand{\thesecapp}{\theapp.\arabic{secapp}}
\renewcommand{\cftsecappfont}{\mdseries} %set font for level 1 entry in loa
\renewcommand{\cftsecapppagefont}{\mdseries} %set front for page numbers

\renewcommand{\cftsecapppresnum}{} %remove word 'Appendix'
\renewcommand{\cftsecappaftersnum}{\hspace{0.5cm}}  %replicate toc format for sub-level-0 headers \thesubappendix (i.e., A.1   title )

\setlength{\cftbeforesecappskip}{0cm} %removes vertical spacing before each chapter element
\renewcommand{\cftsecappafterpnum}{\vskip6pt}
\setlength{\cftsecappindent}{1.55em} %indentation in loa
\settowidth{\cftsecappnumwidth}{\cftsecapppresnum\thesecapp\cftsecappaftersnum\hspace{0.3cm}}

%updates appendix counter, modifies chapter title such so that it is Appendix _letter_: #1
\newcommand{\secapp}[1]{%
  \refstepcounter{secapp}\pdfbookmark[0]{#1}{#1\thesubapp}%
  \section*{\thesecapp\hspace{0.3cm} #1} %spacing between section number and title in text
  \addcontentsline{loa}{secapp}{{\thesecapp\cftsecappaftersnum}#1}%
  \par
}

%leading dots for appendix (end immediately before page number)
\renewcommand{\cftsecappfillnum}[1]{%
 {\cftsecappleader}\nobreak{\cftsecapppagefont #1}\par\cftsecappafterpnum
}


%SUBAPPENDIX (level 2; format A.1.1 : title)
\newlistentry[app]{subapp}{loa}{1}
\renewcommand{\thesubapp}{\thesecapp.\arabic{subapp}}
\renewcommand{\cftsubappfont}{\mdseries} %set font for level 2 entry in loa
\renewcommand{\cftsubapppagefont}{\mdseries} %set front for page numbers

\renewcommand{\cftsubapppresnum}{} %remove word 'Appendix'
\renewcommand{\cftsubappaftersnum}{\hspace{0.5cm}}  %replicate toc format for sub-level-0 headers \thesubappendix (i.e., A.1   title )

\setlength{\cftbeforesubappskip}{0cm} %removes vertical spacing before each chapter element
\renewcommand{\cftsubappafterpnum}{\vskip6pt}
\setlength{\cftsubappindent}{3.10em} %indentation in loa
%\renewcommand{\cftsubappnumwidth}{1.47cm}
\settowidth{\cftsubappnumwidth}{\thesubapp\cftsubappaftersnum\hspace{0.3cm}}

%updates appendix counter, modifies chapter title such so that it is Appendix _letter_: #1
\newcommand{\subapp}[1]{%
  \refstepcounter{subapp}\pdfbookmark[1]{#1}{#1\thesubapp}%
  \subsection*{\thesubapp\hspace{0.3cm} #1}%
  \addcontentsline{loa}{subapp}{{\thesubapp\cftsubappaftersnum}#1}%
  \par
}

%leading dots for appendix (end immediately before page number)
\renewcommand{\cftsubappfillnum}[1]{%
 {\cftsubappleader}\nobreak{\cftsubapppagefont #1}\par\cftsubappafterpnum
}


% SUBSUBAPPENDIX (level 3; format A.1.1.1  title)
\newlistentry[app]{subsubapp}{loa}{1}
\renewcommand{\thesubsubapp}{\thesubapp.\arabic{subsubapp}}
\renewcommand{\cftsubsubappfont}{\mdseries} %set font for level 3 entry in loa
\renewcommand{\cftsubsubapppagefont}{\mdseries} %set front for page numbers


\renewcommand{\cftsubsubapppresnum}{} %remove word 'Appendix'
\renewcommand{\cftsubsubappaftersnum}{\hspace{0.5cm}}  %space after subsubapp title

\setlength{\cftbeforesubsubappskip}{0cm} %removes vertical spacing before each chapter element
\renewcommand{\cftsubsubappafterpnum}{\vskip6pt}
\setlength{\cftsubsubappindent}{4.65em} %indentation in loa (1.55 *2)
\settowidth{\cftsubsubappnumwidth}{\thesubsubapp\cftsubsubappaftersnum\hspace{0.3cm}}

%updates appendix counter, modifies chapter title such so that it is Appendix _letter_: #1
\newcommand{\subsubapp}[1]{%
  \refstepcounter{subsubapp}\pdfbookmark[2]{#1}{#1\thesubsubapp}%
  \subsubsection*{\thesubsubapp\hspace{0.3cm} #1}%
  \addcontentsline{loa}{subsubapp}{{\thesubsubapp\cftsubsubappaftersnum}#1}%
  \par
}

%leading dots for appendix (end immediately before page number)
\renewcommand{\cftsubsubappfillnum}[1]{%
 {\cftsubsubappleader}\nobreak{\cftsubsubapppagefont #1}\par\cftsubsubappafterpnum
}

% PARA (level 4; format A.1.1.1.1  title)
\newlistentry[app]{paraapp}{loa}{1}
\renewcommand{\theparaapp}{\thesubsubapp.\arabic{paraapp}}
\renewcommand{\cftparaappfont}{\mdseries} %set font for level 4 entry in loa
\renewcommand{\cftparaapppagefont}{\mdseries} %set front for page numbers

\renewcommand{\cftparaapppresnum}{} %remove word 'Appendix'
\renewcommand{\cftparaappaftersnum}{\hspace{0.5cm}}  %space after paraapp title

\setlength{\cftbeforeparaappskip}{0cm} %removes vertical spacing before each chapter element
\renewcommand{\cftparaappafterpnum}{\vskip6pt}
\setlength{\cftparaappindent}{6.2em} %indentation in loa (1.55 *2)
\settowidth{\cftparaappnumwidth}{\theparaapp\cftparaappaftersnum\hspace{0.3cm}}

%updates appendix counter, modifies chapter title such so that it is Appendix _letter_: #1
\newcommand{\paraapp}[1]{%
  \refstepcounter{paraapp}\pdfbookmark[3]{#1}{#1\theparaapp}%
  \paragraph*{\theparaapp\hspace{0.3cm} #1}%
  \addcontentsline{loa}{paraapp}{{\theparaapp\cftparaappaftersnum}#1}%
  \par
}

%leading dots for appendix (end immediately before page number)
\renewcommand{\cftparaappfillnum}[1]{%
 {\cftparaappleader}\nobreak{\cftparaapppagefont #1}\par\cftparaappafterpnum
}

% SUBPARA (level 5; format A.1.1.1.1  title)
\newlistentry[app]{subparaapp}{loa}{1}
\renewcommand{\thesubparaapp}{\theparaapp.\arabic{subparaapp}}
\renewcommand{\cftsubparaappfont}{\mdseries} %set font for level 5 entry in loa
\renewcommand{\cftsubparaapppagefont}{\mdseries} %set front for page numbers

\renewcommand{\cftsubparaapppresnum}{} %remove word 'Appendix'
\renewcommand{\cftsubparaappaftersnum}{\hspace{0.5cm}}  %space after subparaapp title

\setlength{\cftbeforesubparaappskip}{0cm} %removes vertical spacing before each chapter element
\renewcommand{\cftsubparaappafterpnum}{\vskip6pt}
\setlength{\cftsubparaappindent}{7.75em} %indentation in loa (1.55 *2)
\settowidth{\cftsubparaappnumwidth}{\thesubparaapp\cftsubparaappaftersnum\hspace{0.3cm}}

%updates appendix counter, modifies chapter title such so that it is Appendix _letter_: #1
\newcommand{\subparaapp}[1]{%
  \refstepcounter{subparaapp}\pdfbookmark[4]{#1}{#1\thesubparaapp}%
  \paragraph*{\thesubparaapp\hspace{0.3cm} #1} %paragraph is used because subparagraph has weird numbering problem
  \addcontentsline{loa}{subparaapp}{{\thesubparaapp\cftsubparaappaftersnum}#1}%
  \par
}

%SUBSUBPARA (level 6; format A.1.1.1.1.1  title)
\newlistentry[app]{subsubparaapp}{loa}{1}
\renewcommand{\thesubsubparaapp}{\thesubparaapp.\arabic{subsubparaapp}}

\renewcommand{\cftsubsubparaapppresnum}{} %remove word 'Appendix'
\renewcommand{\cftsubsubparaappaftersnum}{\hspace{0.5cm}}  %space after subparaapp title

\setlength{\cftbeforesubsubparaappskip}{0cm} %removes vertical spacing before each chapter element
\renewcommand{\cftsubsubparaappafterpnum}{\vskip6pt}
\setlength{\cftsubsubparaappindent}{9.3em} %indentation in loa (1.55 *2)
\settowidth{\cftsubsubparaappnumwidth}{\thesubsubparaapp\cftsubsubparaappaftersnum\hspace{0.3cm}}

%updates appendix counter, modifies chapter title such so that it is Appendix _letter_: #1
\newcommand{\subsubparaapp}[1]{%
  \refstepcounter{subsubparaapp}\pdfbookmark[5]{#1}{#1\thesubsubparaapp}%
  \subparagraph*{\thesubsubparaapp\hspace{0.3cm} #1} %paragraph is used because subparagraph has weird numbering problem
  \addcontentsline{loa}{subsubparaapp}{{\thesubsubparaapp\cftsubsubparaappaftersnum}#1}%
  \par
}

%leading dots for appendix (end immediately before page number)
\renewcommand{\cftsubsubparaappfillnum}[1]{%
 {\cftsubsubparaappleader}\nobreak{\cftsubsubparaapppagefont #1}\par\cftsubsubparaappafterpnum
}

\newcommand{\listabbname}{List of Abbreviations}
\newlistof[chapter]{abb}{loab}{\listabbname} %creates a new appendix counter that will be reset at the start of each \chapter

\setlength{\cftbeforeloabtitleskip}{0cm} %remove vertical space above loab
\setlength{\cftafterloabtitleskip}{0.2cm} %space between title for loab and list entries

\renewcommand{\cftmarkloab}{} %remove header titles

%two lines below ensure centered title for loa
%needed so that table of contents entry is not indented
\renewcommand{\cftloabtitlefont}{\hfill\fontsize{14}{14}\selectfont\bfseries\MakeUppercase}
\renewcommand{\cftafterloabtitle}{\hfill\hfill} %sometimes another \hfill is needed; depends on some setting in above code



%----------------------------------------------------------------------------------------
% REFERENCES & HYPERLINKING
%----------------------------------------------------------------------------------------

\usepackage{hyperref}

\PassOptionsToPackage{backref=true}{biblatex}

\RequirePackage[autocite=inline, style = apa]{biblatex}
\addbibresource{bib/references.bib}


\DeclareSourcemap{\maps[datatype = bibtex]{\map{\step[fieldsource = journal, match = \regexp{\x{26}}, replace = \regexp{\{\\\x{26}\}}] }}}
\DeclareSourcemap{\maps[datatype = bibtex]{\map{\step[fieldsource = title, match = \regexp{\x{26}}, replace = \regexp{\{\\\x{26}\}}] }}}

\hypersetup{pdfpagemode={UseOutlines},
    bookmarksopen=true,
    backref=page}
\usepackage{hypernat}
%%adds escape character to ampersand characters in journal fields of .bib file
\DefineBibliographyStrings{english}{backrefpage={cited on p.},backrefpages={cited on pp.}}




\hypersetup{pdfpagemode={UseOutlines},
bookmarksopen=true, %allows bookmarks in pdf
hypertexnames=true, %enables counting when referencing to sections
colorlinks = true, % Set to true to enable coloring links, a nice option, false to turn them off
%citecolor = blue, % The color of citations
%linkcolor = blue, % The color of references to document elements (sections, figures, etc)
%urlcolor= blue,
%anchorcolor = blue, % The color of hyperlinks (URLs)
allcolors = blue,
pdfstartview={FitV},
breaklinks=true, backref=page
}


%example numbering
\newtheorem{theorem}{Theorem}[section]
\renewcommand{\thetheorem}{\theapp.\arabic{theorem}}
\newtheorem{example}{Example}
\renewcommand{\theexample}{\theapp.\arabic{example}}


%load additional latex packages needed within document
	\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

%----------------------------------------------------------------------------------------
% DOCUMENT OUTLINE
%----------------------------------------------------------------------------------------

% BEGIN DOCUMENT
\begin{document}
\frontmatter %pages will be numbered with roman numerals

  \maketitle

\setcounter{page}{2} %ensures abstract page number starts at roman numberal ii

\cleardoublepage
\thispagestyle{empty} %removes page number only for abstract page
  \begin{abstract}{2}{Despite the value that longitudinal research offers for understanding psychological processes, studies in organizational research rarely use longitudinal designs. One reason for the paucity of longitudinal designs may be the challenges they present for researchers. Three challenges of particular importance are that researchers have to determine 1) how many measurements to take, 2) how to space measurements, and 3) how to design studies when participants provide data with different response schedules (time unstructuredness). In systematically reviewing the simulation literature, I found that few studies comprehensively investigated the effects of measurement number, measurement spacing, and time structuredness (in addition to sample size) on model performance. As a consequence, researchers have little guidance when trying to conduct longitudinal research. To address these gaps in the literature, I conducted a series of simulation experiments. I found poor model performance across all measurement number/sample size pairings. That is, bias and precision were never concurrently optimized under any combination of manipulated variables. Bias was often low, however, with moderate measurement numbers and sample sizes. Although precision was frequently low, the greatest improvements in precision resulted from using either seven measurements with \(N \ge 200\) or nine measurements with \(N \le 100\). With time-unstructured data, model performance systematically decreased across all measurement number/sample size pairings when the model incorrectly assumed an identical response pattern across all participants (i.e., time-structured data). Fortunately, when models were equipped to handle heterogeneous response patterns using definition variables, the poor model performance observed across all measurement number/sample size pairings no longer appeared. Altogether, the results of the current simulation experiments provide guidelines for researchers interested in modelling nonlinear change.}  %[linespacing][abstract][

  \end{abstract}

% notice how yaml variables are indexed with dollar signs and then passed into second argument of preambleItem environments
  \cleardoublepage
  \begin{preambleItem}{2}{Dedication}{{[}To be completed after defence{]}}
  \end{preambleItem}
  \cleardoublepage
   \begin{preambleItem}{2}{Acknowledgements}{{[}To be completed after defence{]}}
  \end{preambleItem}


%move page numbers to top right for list of tables, figures, and tables
\fancypagestyle{plain}{%
  \fancyhf{}% clear all header and footer fields
  \renewcommand{\headrulewidth}{0pt}
  \fancyhead[R]{\thepage}

   }

%table of contents
  \cleardoublepage
  \hypersetup{linkcolor = black, pdfborder= 0 0 0} %remove red borders around toc items
  \setcounter{secnumdepth}{5}
  \setcounter{tocdepth}{5}
  \tableofcontents
  \newpage

%list of tables
  \cleardoublepage
  \listoftables
  \newpage

%list of figures
  \cleardoublepage
  \listoffigures
  \newpage

\cleardoublepage
  \phantomsection
  \addcontentsline{toc}{chapter}{\listabbname}
  \listofabb
\fontsize{12}{12}\selectfont
\begin{table}[h]
    \begin{tabular}{ll}
                 \textbf{ABC} & American Broadcasting Company \\
                 \textbf{CBS} & Colombia Broadcasting System \\
                 \textbf{CUS} & Computer User Services \\
                 \textbf{PBS} & Public Broadcasting System \\
            \end{tabular}
\end{table}
%list of appendices
  \cleardoublepage
  \phantomsection
  \addcontentsline{toc}{chapter}{\listappname}
  \listofapp

  \newpage

\mainmatter % here the regular arabic numbering starts

\nocite{R-tidyverse, R-nonlinSims, R-nonlinSimsAnalysis, R-devtools, R-RColorBrewer, R-cowplot, R-data.table, R-egg, R-ggbrace, R-ggtext, R-kableExtra, R-knitr}

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}
\begin{quote}
    ``Neither the behavior of human beings nor the activities of organizations can be defined without reference to time, and temporal aspects are critical for understanding them" \parencite[][p. 136]{navarro2015}.
\end{quote}
The topic of time has received considerable attention in organizational psychology over the past 20 years. Examples of well-received articles published around the beginning of the 21\textsuperscript{st} century have discussed how investigating time is important for
understanding patterns of change and boundary conditions of theory
\autocite{zaheer1999}, how longitudinal research is necessary for disentangling
different types of causality \autocite{mitchell2001}, and explicated patterns
of organizational change \autocite[or institutionalization;][]{lawrence2001}.
Since then, articles have emphasized the need to address time in
specific areas such as performance \autocite{fisher2008,dalal2014}, teams \autocite{roe2012}, and goal setting \autocite{fried2004} and, more generally, throughout organizational research \autocite{george2000,roe2008,ployhart2010,sonnentag2012,navarro2015,shipp2015,kunisch2017,vantilborgh2018,aguinis2021}.

The importance of time has also been recognized in organizational theory. In defining a theoretical contribution, \textcite{whetten1989} discussed that time must be discussed in setting boundary conditions (i.e., under what circumstances does the theory apply) and in specifying relations between variables over time \autocite{mitchell2001,george2000}. Even if a considerable number of organizational theories do not adhere to the definition of \textcite{whetten1989}, theoretical models in organizational psychology consist of path diagrams that delineate the causal events of processes. Given that temporal precedence is a necessary condition for establishing causality \autocite{mill2011}, time has a role, whether implicitly or explicitly, in organizational theory.

Despite the considerable attention given towards investigating processes over time and the ubiquity of time in organizational theory, the prevalence of longitudinal research has historically remained low. One study examined the prevalence of longitudinal research from 1970--2006 across five organizational psychology journals and found that 4\% of articles used longitudinal designs \autocite{roe2014b}. Another survey of two applied psychology journals in 2005 found that approximately 10\% (10 of 105 studies) of studies used longitudinal designs \autocite{roe2008}. Similarly, two surveys of studies employing longitudinal designs with mediation analysis found that, across five journals, only about 10\% (7 of 72 studies) did so in 2005 \autocite{maxwell2007} and approximately 16\% (15 of 92 studies) did so in 2006 \autocite{mitchell2013}.\footnote{Note that the definition of a longitudinal design in \textcite{maxwell2007} and \textcite{mitchell2013} required that measurements be taken over at least three time points so that measurements of the predictor, mediator, and outcome variables were separated over time.} Thus, the prevalence of longitudinal research has remained low.

In the seven sections that follow, I will explain why longitudinal research is necessary and the factors that must be considered when conducting such research. In the first section, I will explain why conducting longitudinal research is essential for understanding the dynamics of psychological processes. In the second section, I will overview patterns of change that are likely to emerge over time. In the third section, I will overview design and analytical issues involved in conducting longitudinal studies. In the fourth section, I will explain how design and analytical issues encountered in conducting longitudinal research can be investigated. In the fifth section, I will provide a systematic review of the research that has investigated design and analytical issues involved in conducting longitudinal research. Finally, in the sixth and seventh sections, I will, respectively, discuss some methods for modelling nonlinear change and the frameworks in which they can be used. A summary of the three simulation experiments that I conducted in my dissertation will then be provided.

\hypertarget{the-need-to-conduct-longitudinal-research}{%
\section{The Need to Conduct Longitudinal Research}\label{the-need-to-conduct-longitudinal-research}}

Longitudinal designs provide several advantages over cross-sectional designs that allow them to more accurately investigate change (e.g., temporal precedence, testing reverse causality). Unfortunately, even though longitudinal studies often produce results that differ from those of cross-sectional studies, researchers commonly discuss the results of cross-sectional studies as if they have been obtained with a longitudinal design. One example of the assumption of equivalence between cross-sectional and longitudinal findings comes from the large number of studies employing mediation analysis. Given that mediation is used to understand chains of causality in psychological processes \autocite{baron1986}, it would thus make sense to pair mediation analysis with a longitudinal design because understanding causality, after all, requires temporal precedence. Unfortunately, the majority of studies that have used mediation analysis have done so using cross-sectional designs---with estimates of approximately 90\% \autocite{maxwell2007} and 84\% \autocite{mitchell2013}---and often discuss the results as if they are longitudinal. Investigations into whether mediation results remain equivalent across cross-sectional and longitudinal designs have repeatedly concluded that using mediation analysis on cross-sectional data can return different, and sometimes completely opposite, results from using it on longitudinal data \autocite{cole2003,maxwell2007,maxwell2011,mitchell2013,olaughlin2018}. Therefore, mediation analyses based on cross-sectional analyses may be misleading.

The non-equivalence of cross-sectional and longitudinal results that occurs with mediation analysis is, unfortunately, not due to a specific set of circumstances that only arise with mediation analysis, but a consequence of a broader systematic cause that affects the results of many analyses. The concept of ergodicity explains why cross-sectional and longitudinal analyses seldom yield similar results. To understand ergodicity, it is first important to realize that variance is central to many statistical analyses---correlation, regression, factor analysis, and mediation are some examples. Thus, if variance remains unchanged across cross-sectional and longitudinal data sets, then analyses of either data set would return the same results. Importantly, variance only remains equal across cross-sectional and longitudinal data sets if two conditions put forth by ergodic theory are satisfied \autocites[homogeneity and stationarity;][]{molenaar2004,molenaar2009}. If these two conditions are met, then a process is said to be ergodic. Unfortunately, the two conditions required for ergodicity are highly unlikely to be satisfied and so cross-sectional findings will frequently deviate from longitudinal findings (for a detailed discussion, see Appendix \ref{ergodicity}).

Given that cross-sectional and longitudinal analyses are, in general, unlikely to return equivalent findings, it is unsurprising that several investigations in organizational research---and psychology as a whole---have found these analyses to return different results. Beginning with an example from \textcite{curran2011}, heart attacks are less likely to occur in people who exercise regularly (longitudinal finding), but more likely to happen when exercising (cross-sectional finding). Correlational studies find differences in correlation magnitudes between cross-sectional and longitudinal data sets \autocites[for a meta-analytic review, see][]{nixon2011,fisher2018}.\footnote{Note that \textcite{fisher2018} also found the variability of longitudinal correlations to be considerably larger than the variability of cross-sectional correlations.} Moving on to perhaps the most commonly employed analysis in organizational research of mediation, several articles have highlighted that cross-sectional data can return different, and sometimes completely opposite, results than those obtained from longitudinal data \autocite{cole2003,maxwell2007,maxwell2011,olaughlin2018}. Factor analysis is perhaps the most interesting example: The well-documented five-factor model of personality seldom arises when analyzing person-level data obtained by measuring personality on 90 consecutive days \autocite{hamaker2005}. Therefore, cross-sectional analyses are rarely equivalent to longitudinal analyses.

With longitudinal analyses often producing results that differ from those of cross-sectional analyses, it is paramount that longitudinal designs be used to more accurately understand change. Fortunately, technological advancements have allowed researchers to more easily conduct longitudinal research in two ways. First, the use of the experience sampling method \autocite{beal2015} in conjunction with modern information transmission technologies---whether through phone applications or short message services---allows data to often be sampled over time with relative ease. Second, the development of longitudinal analyses (along with their integration in commonly used software) that enable person-level data to be modelled such as multilevel models \autocite{raudenbush2002}, growth mixture models \autocite{wang2007}, and dynamic factor analysis \autocite{ram2013} provide researchers with avenues to explore the temporal dynamics of psychological processes. With one recent survey estimating that 43.3\% of mediation studies (26 of 60 studies) used a longitudinal design \autocite{olaughlin2018}, it appears that the prevalence of longitudinal research has increased from the 9.5\% \autocite{roe2008} and 16.3\% \autocite{mitchell2013} values estimated at the beginning of the 21\textsuperscript{st} century. Although the frequency of longitudinal research appears to have increased over the past 20 years, several avenues exist where the quality of longitudinal research can be improved, and in my dissertation, I focus on investigating these avenues.

\hypertarget{understanding-patterns-of-change-that-emerge-over-time}{%
\section{Understanding Patterns of Change That Emerge Over Time}\label{understanding-patterns-of-change-that-emerge-over-time}}

Change can occur in many ways over time. One pattern of change commonly assumed to occur over time is that of linear change. When change follows a linear pattern, the rate of change over time remains constant. Unfortunately, a linear pattern places demanding restrictions on the possible trajectories of change. If change were to follow a linear pattern, then any pauses in change (or plateaus) or changes in direction could not occur: Change would simply grow over time. Unfortunately, effect sizes have been shown to diminish over time after peaking \autocites[for meta-analytic examples, see][]{cohen1993,griffeth2000,hom1992,riketta2008,steel1984,steel1990}. Moreover, many variables display cyclic patterns of change over time, with mood \autocite{larsen1990}, daily stress \autocite{bodenmann2010}, and daily drinking behaviour \autocite{huh2015} as some examples. Therefore, change over is unlikely to follow a linear pattern.

A more realistic pattern of change to occur over time is a nonlinear pattern \autocite[for a review, see][]{cudeck2007}. Nonlinear change allows the rate of change to be nonconstant; that is, change may occur more rapidly during certain periods of time, stop altogether, or reverse direction. When looking at patterns of change observed across psychology, several examples of nonlinear change have been found in the declining rate of speech errors throughout child development \autocite{burchinal1991}, rates of forgetting \autocite{murre2015}, development of habits \autocite{fournier2017}, and the formation of opinions \autocite{xia2020}. Given that nonlinear change appears more likely than linear change, my dissertation will assume change over time to be nonlinear.

\hypertarget{challenges-involved-in-conducting-longitudinal-research}{%
\section{Challenges Involved in Conducting Longitudinal Research}\label{challenges-involved-in-conducting-longitudinal-research}}

Conducting longitudinal research presents researchers with several challenges. Many challenges are those from cross-sectional research only amplified \autocite[for a review, see][]{bergman1993}.\footnote{It should be noted that conducting a longitudinal study does alleviate some issues encountered in conducting cross-sectional research. For example, taking measurements over multiple time points likely reduces common method variance \parencites{podsakoff2003}[for an example, see ][]{ostroff2002}.} For example, greater efforts have to be made to to prevent missing data which can increase over time \autocite{newman2008,dillman2014}. Likewise, the adverse effects of well-documented biases such as demand characteristics \autocite{orne1962} and social desirability \autocite{nederhof1985} have to be countered at each time point. Outside of challenges shared with cross-sectional research, conducting longitudinal research also presents new challenges. Analyses of longitudinal data have to consider complications such as how to model error structures \autocite{grimm2010a}, check for measurement non-invariance over time \autocite[the extent to which a construct is measured with the same measurement model over time;][]{mellenbergh1989}, and how to center/process data to appropriately answer research questions \autocite{enders2007,wang2015}.

Although researchers must contend with several issues in conducting longitudinal research, three issues are of particular interest in my dissertation. The first issue concerns how many measurements to use in a longitudinal design. The second issue concerns how to space the measurements. The third issue focuses on how much error is incurred if the time structuredness of the data is overlooked. The sections that follow will review each of these issues.

\hypertarget{number-of-measurements}{%
\subsection{Number of Measurements}\label{number-of-measurements}}

Researchers have to decide on the number of measurements to include in a longitudinal study. Although using more measurements increases the accuracy of results---as noted in the results of several studies \autocites[e.g.,][]{coulombe2016,timmons2015,finch2017,fine2019}---taking additional measurements often comes at a cost that a researcher may be unable to absorb given a limited budget. One important point to mention is that a researcher designing a longitudinal study must take at least three measurements to allow a reliable estimate of change and, perhaps more importantly, to allow a nonlinear pattern of change to be modelled \autocite{ployhart2010}. In my dissertation, I hope to determine whether an optimal number of measurements exists when modelling a nonlinear pattern of change.

\hypertarget{spacing-of-measurements}{%
\subsection{Spacing of Measurements}\label{spacing-of-measurements}}

Additionally, a researcher must decide on the spacing of measurements in a longitudinal study. Although discussions of measurement spacing often recommend that researchers use theory and previous studies to determine measurement spacing \autocite{mitchell2001,cole2003,collins2006,dormann2014,dormann2015}, organizational theories seldom delineate periods of time over which a processes unfold, and so the majority of longitudinal research uses intervals of convention and/or convenience to space measurements \autocite{mitchell2001,dormann2014}. Unfortunately, using measurement spacings that do not account for the temporal pattern of change of a psychological process can lead to inaccurate results \autocite[e.g.,][]{chen2014}. As an example, \textcite{cole2009} show how correlation magnitudes are affected by the choice of measurement spacing intervals. In my dissertation, I hope to determine whether an optimal measurement spacing schedule exists when modelling a nonlinear pattern of change.

\hypertarget{time-structuredness}{%
\subsection{Time Structuredness}\label{time-structuredness}}

Last, and perhaps most pernicious, latent variable analyses of longitudinal data are likely to incur error from an assumption they make about data collection conditions. Latent variable analyses assume that, across all collection points, participants provide their data at the same time. Unfortunately, such a high level of regularity in the response patterns of participants is unlikely: Participants are more likely to provide their data over some period of time after a data collection window has opened. As an example, consider a study that collects data from participants at the beginning of each month. If participants respond with perfect regularity, then they would all provide their data at the exact same time (e.g., noon on the second day of each month). If the participants respond with imperfect regularity, then they would provide their at different times after the beginning of each month. The regularity of response patterns observed across participants in a longitudinal study determines the time structuredness of the data and the sections that follow will provide overview of time structuredness.

\hypertarget{time-structured-data}{%
\subsubsection{Time-Structured Data}\label{time-structured-data}}

Many analyses assume that data are \emph{time structured}: Participants provide data at the same time at each collection point. By assuming time-structured data, an analysis can incur error because it will map time intervals of inappropriate lengths onto the time intervals that occurred between participant's responses.\footnote{It should be noted that, although seldom implemented, analyses can be accessorized to handle time-unstructured data by using definition variables \parencites{mehta2000}{mehta2005}.} As an example of the consequences of incorrectly assuming data to be time structured, consider a study that assessed the effects of an intervention on the development of leadership by collecting leadership ratings at four time points each separated by four weeks \autocite{day2011}. The employed analysis assumed time-structured data; that is, each each participant provided ratings on the same day---more specifically, the exact same moment---each time these ratings were collected. Unfortunately, it is unlikely that the data collected from participants were time structured: At any given collection point, some participants may have provided leadership ratings at the beginning of the week, while others may only provide ratings two weeks after the survey opened. Importantly, ratings provided two weeks after the survey opened were likely influenced by changes in leadership that occurred over the two weeks. If an analysis incorrectly assumes time-structured data, then it assumes each participant has the same response pattern and, therefore, will incorrectly attribute the amount of time that elapses between most participants' responses. For instance, if a participant only provides a leadership rating two weeks after having received a survey (and six weeks after providing their previous rating), then using an analysis that assumes time-structured data would incorrectly assume that each collection point of this participant is separated by four weeks (the interval used in the experiment) and would, consequently, model the observed change as if it had occurred over four weeks. Therefore, incorrectly assuming data to be time structured leads an analysis to overlook the unique response rates of participants across the collection points and, as a consequence, incur error \autocite{mehta2000,mehta2005,coulombe2016}.

\hypertarget{time-unstructured-data}{%
\subsubsection{Time-Unstructured Data}\label{time-unstructured-data}}

Conversely, other analyses assume that data are \emph{time unstructured}: Participants provide data at different times at each collection point. Given the unlikelihood of one response pattern describing the response rates of all participants in a given study, the data obtained in a study are unlikely to be time structured. Instead, and because participants are likely to exhibit unique response patterns in their response rates, data are likely to be time unstructured. One way to conceptualize the distinction between time-structured and time-unstructured data is on a continuum. On one end of the continuum, participants all provide data with identical response patterns, thus giving time-structured data. When participants exhibit unique response patterns, the resulting data are time unstructured, with the extent of time-unstructuredness depending on the average uniqueness of all response patterns. For example, if data are collected at the beginning of each month and participants only have one day to provide data at each time point, then the resulting data will have a low amount of time structuredness because response patterns can only differ from each other over the course of one day. Alternatively, if data are collected at the beginning of each month and participants have 30 days to provide data at each time point, then the resulting data will have a high amount of time structuredness because response patterns can differ from each other over the course of 30 days. Therefore, the continuum of time struturedness has time-structured data on one end and time-unstructured data with long response windows on another end. In my dissertation, I hope to determine how much error is incurred when time-unstructured data of varying degrees are assumed to be time structured.

\hypertarget{summary}{%
\subsection{Summary}\label{summary}}

In summary, researchers must contend with several issues when conducting longitudinal research. In addition to contending with issues encountered in conducting cross-sectional research, researchers must contend with new issues that arise from conducting longitudinal research. Three issues of particular importance in my dissertation are the number of measurements, the spacing of measurements, and incorrectly assuming time-unstructured data to be time structured. These issues will be serve as a basis for a systematic review of the simulation literature.

\hypertarget{using-simulations-to-assess-modelling-accuracy}{%
\section{Using Simulations To Assess Modelling Accuracy}\label{using-simulations-to-assess-modelling-accuracy}}

In the next section, I will present the results of a systematic review of the literature that has investigated the issues of measurement number, measurement spacing, and time structuredness. Before presenting the results of the systematic review, I will provide an overview of the Monte Carlo method used to investigate the issues involved in conducting longitudinal research.

To understand how the effects of longitudinal issues on modelling accuracy can be investigated, the inferential method commonly employed in psychological research will first be reviewed with an emphasis on its shortcomings (see Figure \ref{fig:MonteCarlo-comparison}). Consider an example where a researcher wants to understand how sampling error affects the accuracy with which a sample mean (\(\bar{x}\)) estimates a population mean (\(\upmu\)). Using the inferential method, the researcher samples data and then estimates the population mean (\(\upmu\)) by computing the mean of the sampled data (\(\bar{x}_1\)). Because collected samples are almost always contaminated by a variety of methodological and/or statistical deficiencies (such as sampling error, measurement error, assumption violations, etc.), the estimation of the population parameter is likely to be imperfect. Unfortunately, to estimate the effect of sampling error on the accuracy of the population mean estimate (\(\bar{x}_1\)), the researcher would need to know the value of the population mean; without knowing the value of the population mean, it is impossible to know how much error was incurred in estimating the population mean and, as as a result, impossible to know the extent to which sampling error contributed to this error. Therefore, a study following the inferential approach can only provide estimates of population parameters.

The Monte Carlo method has a different goal. Whereas the inferential method focuses on estimating parameters from sample data, the Monte Carlo method is used to understand the factors that influence the accuracy of the inferential approach. Figure \ref{fig:MonteCarlo-comparison} shows that the Monte Carlo method works in the opposite direction of the inferential approach: Instead of collecting a sample, the Monte Carlo method begins by assigning a value to at least one parameter to define a population. Many sample data sets are then generated from the defined population (\(s_1, s_2, ..., s_n\)) and the data from each sample are then modelled by computing a sample mean (\(\bar{x}_1, \bar{x}_2, ..., \bar{x}_n\)). Importantly, manipulations can be applied to the sampling and/or modelling of the data. In the current example,the population estimates of each statistical model are averaged (\(\bar{\bar{x}}\)) and compared to the pre-determined parameter value (\(\upmu\)). The difference between the average of the estimates and the known population value constitutes bias in parameter estimation (i.e., parameter bias). In the current example, the manipulation causes a systematic underestimation, on average, of the population parameter. By randomly generating data, the Monte Carlo method can estimate how a variety of methodological and statistical factors affect the accuracy of a model \autocite[for a review, see][]{robert2010}.

Monte Carlo simulations have been used to evaluate the effects of a variety of methodological and statistical deficiencies for several decades. Beginning with an early use of the Monte Carlo method, \textcite{boneau1960} used it to evaluate the effects of assumption violations on the fidelity of \emph{t}-value distributions. In more recent years, implementations of the the Monte Carlo method have shown that realistic values of sample size
and measurement accuracy produce considerable variability in estimated correlation values \autocite{stanley2014}. Monte Carlo simulations have also provided valuable insights into more complicated statistical analyses. In investigating more complex statistical analyses, simulations have shown that mediation analyses are biased to produce results of complete mediation because the statistical power to detect direct effects falls well below the statistical power to detect indirect effects \autocite{kenny2014}. Given the ability of the Monte Carlo method to evaluate statistical methods, the experiments in my dissertation used it to evaluate the effects of measurement number, measurement spacing, and time structuredness on modelling accuracy.\footnote{My simulation experiments also investigated the effects of sample size and nature of change on modelling accuracy.}

\hypertarget{systematic-review-of-simulation-literature}{%
\section{Systematic Review of Simulation Literature}\label{systematic-review-of-simulation-literature}}

To understand the extent to which issues involved in conducting longitudinal research had been investigated, I conducted a systematic review of the simulation literature.
\begin{apaFigure}
[landscape]
[samepage]
[0cm]
{Depiction of Monte Carlo Method}
{MonteCarlo-comparison}
{0.7}
{Figures/Monte_Carlo_comparison}
{Comparison of inferential approach with the Monte Carlo approach. The inferential approach begins with a collected sample and then estimates the population parameter using an appropriate statistical model. The difference between the estimated and population value can be conceptualized as error. Because the population value is generally unknown in the inferential approach, it cannot estimate how much error is introduced by any given methodological or statistical deficiency. To estimate how much error is introduced by any given methodological or statistical deficiency, the Monte Carlo method needs to be used, which constitutes four steps. The Monte Carlo method first defines a population by setting parameter values. Second, many samples are generated from the pre-defined population, with some methodological deficiency built in to each data set (in this case, each sample has a specific amount of missing data). Third, each generated sample is then analyzed and the population estimates of each statistical model are averaged and compared to the pre-determined parameter value. Fourth, the difference between the estimate average and the known population value defines the extent to which the missing data manipulation affected parameter estimation (the difference between the population and average estimated population value is the parameter bias).}
\end{apaFigure}
The sections that follow will first present the method I followed in systematically reviewing the literature and then summarize the findings of the review.

\hypertarget{systematic-review-methodology}{%
\subsection{Systematic Review Methodology}\label{systematic-review-methodology}}

I identified the following keywords through citation searching and independent reading: ``growth curve'', ``time-structured analysis'', ``time structure'', ``temporal design'', ``individual measurement occasions'', ``measurement intervals'', ``methods of timing'', ``longitudinal data analysis'', ``individually-varying time points'', ``measurement timing'', ``latent difference score models'', ``parameter bias'', and ``measurement spacing''. I entered these keywords entered into the PsycINFO database (on July 23, 2021) along with the word ``simulation'' in any field and considered any returned paper a viable ppaper (see Figure \ref{fig:prismaDiagram} for a PRISMA diagram illustrating the filtering of the reports). The search returned 165 reports, which I screened by reading the abstracts. Initial screening led to the removal of 60 reports because they did not contain any simulation experiments. Of the remaining 105 papers, I removed 2 more papers because they could not be accessed \autocite{stockdale2007,tiberio2008}. Of the remaining 103 identified simulation studies, I deemed a paper as relevant if it investigated the effects of any design and/or analysis factor related to conducting longitudinal research (i.e., number of measurements, spacing of measurements, and/or time structuredness) and did so using the Monte Carlo simulation method. Of the remaining 103 studies, I removed 89 studies because they did not meet the inclusion criteria, leaving fourteen studies to be included in the review. I also found an additional 3 studies through citation searching, giving a total of 17 studies.

The findings of my systematic review are summarized in Tables \ref{tab:systematicReviewCount}--\ref{tab:systematicReview}. Tables \ref{tab:systematicReviewCount}--\ref{tab:systematicReview} differ in one way: Table \ref{tab:systematicReviewCount} indicates how many studies investigated each effect, whereas Table \ref{tab:systematicReview} provides the reference of each study and detailed information about each study's method. Otherwise, all other details of Tables \ref{tab:systematicReviewCount}--\ref{tab:systematicReview} are identical. The first column lists the longitudinal design factor (alongside with sample size) and the corresponding two- and three-way interactions. The second and third columns list whether each effect has been investigated with linear and nonlinear patterns of change, respectively. Shaded cells indicate effects that have not been investigated, with cells shaded in light grey indicating effects that have not been investigated with linear patterns of change and cells shaded in dark grey indicating effects that have not been investigated with nonlinear patterns of change.\footnote{Table \ref{tab:systematicReview} lists the effects that each study (identified by my systematic review) investigated and notes the following methodological details (using superscript letters and symbols): the type
of model used in each paper, assumption and/or manipulation of complex error structures
(heterogeneous variances and/or correlated residuals), manipulation of missing data,
and/or pseudo-time structuredness manipulation. Across all 17 simulation studies, 5 studies (29\%) assumed complex error structures \parencites{gasimova2014}{liu2021}{liu2015}{miller2017}{murphy2011}, 1 study (6\%) manipulated missing data \parencite{fine2019}, and 2 studies (12\%) contained a pseudo-time structuredness manipulation \parencites{fine2019}{fine2020}. Importantly, the pseudo-time structuredness manipulation used in \textcite{fine2019} and \textcite{fine2020} differed from the manipulation of time structuredness used in the current experiments \parencites[and from previous simulation experiments of][]{coulombe2016}{miller2017} in that it randomly generated longitudinal data such that a given person could provide all their data before another person provided any data.}

\hypertarget{systematic-review-results}{%
\subsection{Systematic Review Results}\label{systematic-review-results}}

Although previous research appeared to sufficiently fill some cells of Table \ref{tab:systematicReviewCount}, two patterns suggest that arguably the most important cells (or effects) have not been investigated. First, it appears that simulation research has invested more effort in investigating the effects of longitudinal design factors with linear patterns than with nonlinear patterns of change. In counting the number of effects that remain unaddressed with linear and nonlinear patterns of change, a total of five cells (or effects) have not been investigated, but a total of seven cells have not been investigated with nonlinear patterns of
\begin{apaFigure}
[landscape]
{PRISMA Diagram Showing Study Filtering Strategy}
{prismaDiagram}
{0.7}
{Figures/prisma_diagram}
{PRISMA diagram for systematic review of simulation research that investigates longitudinal design and analysis factors.}
\end{apaFigure}
\newgeometry{margin=2.54cm}
\begin{landscape}
\begin{ThreePartTable}
\begin{TableNotes}
\item \textit{Note. }Cells are only numbered for effects that have not been investigated. Cells shaded in light grey indicate effects that have not been investigated with linear patterns of change and cells shaded in dark grey indicate effects that have not been investigated with nonlinear patterns of change.
\end{TableNotes}
\begin{longtable}[l]{>{\raggedright\arraybackslash}p{4.5cm}>{\centering\arraybackslash}p{8cm}>{\centering\arraybackslash}p{8cm}}
\caption{\label{tab:systematicReviewCount}Number of Simulation Studies That Have Investigated Longitudinal Issues with Linear and Nonlinear Change Patterns (\textit{n} = 17)}\\
\toprule
Effect & Linear pattern & Nonlinear pattern\\
\midrule
\endfirsthead
\caption[]{\label{tab:systematicReviewCount}Number of Simulation Studies That Have Investigated Longitudinal Issues with Linear and Nonlinear Change Patterns (\textit{n} = 17) \textit{(continued)}}\\
\toprule
Effect & Linear pattern & Nonlinear pattern\\
\midrule
\endhead

\endfoot
\bottomrule
\insertTableNotes
\endlastfoot
\textbf{Main effects} & \cellcolor{white}{} & \cellcolor{white}{}\\
\cmidrule{1-3}
Number of measurements (NM) & \cellcolor{white}{11 studies} & \cellcolor{white}{6 studies}\\
 
Spacing of measurements (SM) & \cellcolor{white}{1 study} & \cellcolor{white}{1 study}\\
 
Time structuredness (TS) & \cellcolor{white}{2 studies} & \cellcolor{white}{1 study}\\
 
Sample size (S) & \cellcolor{white}{11 studies} & \cellcolor{white}{7 studies}\\
\cmidrule{1-3}
\textbf{Two-way interactions} & \cellcolor{white}{} & \cellcolor{white}{}\\
\cmidrule{1-3}
NM x SM & \cellcolor{white}{1 study} & \cellcolor{white}{1 study}\\
 
NM x TS & \cellcolor{white}{1 study} & \cellcolor[HTML]{C7C4C4}{\textbf{Cell 1 (\hyperref[Exp3]{Exp. 3})}}\\
 
NM x S & \cellcolor{white}{9 studies} & \cellcolor{white}{5 studies}\\
 
SM x TS & \cellcolor[HTML]{E4E2E2}{\textbf{Cell 2}} & \cellcolor[HTML]{C7C4C4}{\textbf{Cell 3}}\\
 
SM x S & \cellcolor[HTML]{E4E2E2}{\textbf{Cell 4}} & \cellcolor[HTML]{C7C4C4}{\textbf{Cell 5 (\hyperref[Exp2]{Exp. 2})}}\\
 
TS x S & \cellcolor{white}{1 study} & \cellcolor{white}{2 studies}\\
\cmidrule{1-3}
\textbf{Three-way interactions} & \cellcolor{white}{} & \cellcolor{white}{}\\
\cmidrule{1-3}
NM x SM x TS & \cellcolor[HTML]{E4E2E2}{\textbf{Cell 6}} & \cellcolor[HTML]{C7C4C4}{\textbf{Cell 7}}\\
 
NM x SM x S & \cellcolor[HTML]{E4E2E2}{\textbf{Cell 8}} & \cellcolor[HTML]{C7C4C4}{\textbf{Cell 9 (\hyperref[Exp2]{Exp. 2})}}\\
 
NM x TS x S & \cellcolor{white}{1 study} & \cellcolor[HTML]{C7C4C4}{\textbf{Cell 10 (\hyperref[Exp3]{Exp. 3})}}\\
 
SM x TS x S & \cellcolor[HTML]{E4E2E2}{\textbf{Cell 11}} & \cellcolor[HTML]{C7C4C4}{\textbf{Cell 12}}\\*
\end{longtable}
\end{ThreePartTable}
\end{landscape}
\restoregeometry

\newgeometry{margin=2.54cm}
\begin{landscape}
\begin{ThreePartTable}
\begin{TableNotes}
\item \textit{Note. }Cells are only numbered for effects that have not been investigated. Cells shaded in light and dark grey indicate effects that have not, respectively, been investigated with linear and nonlinear patterns of change.
\item[a] Latent growth curve model. \textsuperscript{b} Second-order latent growth curve model. \textsuperscript{c} Hierarchical Bayesian model. \textsuperscript{d} Bivariate latent change score model. \textsuperscript{e} Functional mixed-effects model. \textsuperscript{f} Nonlinear mixed-effects model. \textsuperscript{g} Bilinear spline model. \textsuperscript{g} Parallel bilinear spline model.
\item[$\circ$] Manipulated missing data. $^\mho$ Assumed complex error structure (heterogeneous variances and/or correlated residuals). $^\triangledown$ Contained pseudo-time structuredness manipulation.
\end{TableNotes}
\begin{longtable}[l]{l>{\centering\arraybackslash}p{8cm}>{\centering\arraybackslash}p{8cm}}
\caption{\label{tab:systematicReview}Summary of Simulation Studies That Have Investigated Longitudinal Issues with Linear and Nonlinear Change Patterns (\textit{n} = 17)}\\
\toprule
Effect & Linear pattern & Nonlinear pattern\\
\midrule
\endfirsthead
\caption[]{\label{tab:systematicReview}Summary of Simulation Studies That Have Investigated Longitudinal Issues with Linear and Nonlinear Change Patterns (\textit{n} = 17) \textit{(continued)}}\\
\toprule
Effect & Linear pattern & Nonlinear pattern\\
\midrule
\endhead

\endfoot
\bottomrule
\insertTableNotes
\endlastfoot
\textbf{Main effects} & \cellcolor{white}{} & \cellcolor{white}{}\\
\cmidrule{1-3}
Number of measurements (NM) & \cellcolor{white}{\parencites[][\textsuperscript{a}]{timmons2015}[][\textsuperscript{b}$^{\mho}$]{murphy2011}[][\textsuperscript{c}$^{\mho}$]{gasimova2014}[][\textsuperscript{a}]{wu2014}[][\textsuperscript{a}]{coulombe2016b}[][\textsuperscript{a}]{ye2016}[][\textsuperscript{a}]{finch2017}[][\textsuperscript{d}]{orourke2021}[][\textsuperscript{a}]{newsom2020}[][\textsuperscript{a}]{coulombe2016}} & \cellcolor{white}{\parencites[][\textsuperscript{a}]{timmons2015}[][\textsuperscript{a}]{finch2017}[][\textsuperscript{e}$^{\circ\triangledown}$]{fine2019}[][\textsuperscript{e,f}$^{\triangledown}$]{fine2020}[][\textsuperscript{g}]{liu2022}[][\textsuperscript{h}$^{\mho}$]{liu2021}[][\textsuperscript{g}$^{\mho}$]{liu2015}}\\
 
Spacing of measurements (SM) & \cellcolor{white}{\parencite[][\textsuperscript{a}]{timmons2015}} & \cellcolor{white}{\parencite[][\textsuperscript{a}]{timmons2015}}\\
 
Time structuredness (TS) & \cellcolor{white}{\parencites[][\textsuperscript{a}]{aydin2014}[][\textsuperscript{a}]{coulombe2016}} & \cellcolor{white}{\parencites[][\textsuperscript{a}$^{\mho}$]{miller2017}[][\textsuperscript{g}$^{\mho}$]{liu2015}}\\
 
Sample size (S) & \cellcolor{white}{\parencites[][\textsuperscript{b}${\mho}$]{murphy2011}[][\textsuperscript{c}$^{\mho}$]{gasimova2014}[][\textsuperscript{a}]{wu2014}[][\textsuperscript{a}]{coulombe2016b}[][\textsuperscript{a}]{ye2016}[][\textsuperscript{a}]{finch2017}[][\textsuperscript{d}]{orourke2021}[][\textsuperscript{a}]{newsom2020} [][\textsuperscript{a}]{coulombe2016}[][\textsuperscript{a}]{aydin2014}} & \cellcolor{white}{\parencites[][\textsuperscript{a}]{finch2017}[][\textsuperscript{e}$^{\circ\triangledown}$]{fine2019}[][\textsuperscript{e,f}$^{\triangledown}$]{fine2020}[][\textsuperscript{g}]{liu2022}[][\textsuperscript{h}$^{\mho}$]{liu2021}[][\textsuperscript{g}$^{\mho}$]{liu2015}[][\textsuperscript{a}$^{\mho}$]{miller2017}}\\
\cmidrule{1-3}
\textbf{Two-way interactions} & \cellcolor{white}{} & \cellcolor{white}{}\\
\cmidrule{1-3}
NM x SM & \cellcolor{white}{\parencite[][\textsuperscript{a}]{timmons2015}} & \cellcolor{white}{\parencite[][\textsuperscript{a}]{timmons2015}}\\
 
NM x TS & \cellcolor{white}{\parencite[][\textsuperscript{a}]{coulombe2016}} & \cellcolor[HTML]{C7C4C4}{\textbf{Cell 1 (\hyperref[Exp3]{Exp. 3})}}\\
 
NM x S & \cellcolor{white}{\parencites[][\textsuperscript{b}${\mho}$]{murphy2011}[][\textsuperscript{c}$^{\mho}$]{gasimova2014}[][\textsuperscript{a}]{wu2014}[][\textsuperscript{a}]{coulombe2016b}[][\textsuperscript{a}]{ye2016}[][\textsuperscript{a}]{finch2017}[][\textsuperscript{d}]{orourke2021} [][\textsuperscript{a}]{newsom2020}[][\textsuperscript{a}]{coulombe2016}} & \cellcolor{white}{\parencites[][\textsuperscript{a}]{finch2017}[][\textsuperscript{e}$^{\circ\triangledown}$]{fine2019}[][\textsuperscript{e,f}$^{\triangledown}$]{fine2020}[][\textsuperscript{g}]{liu2022}[][\textsuperscript{h}$^{\mho}$]{liu2021}}\\
 
SM x TS & \cellcolor[HTML]{E4E2E2}{\textbf{Cell 2}} & \cellcolor[HTML]{C7C4C4}{\textbf{Cell 3}}\\
 
SM x S & \cellcolor[HTML]{E4E2E2}{\textbf{Cell 4}} & \cellcolor[HTML]{C7C4C4}{\textbf{Cell 5 (\hyperref[Exp2]{Exp. 2})}}\\
 
TS x S & \cellcolor{white}{\parencite[][\textsuperscript{a}]{aydin2014}} & \cellcolor{white}{\parencites[][\textsuperscript{g}$^{\mho}$]{liu2015}[][\textsuperscript{a}$^{\mho}$]{miller2017}}\\
\cmidrule{1-3}
\textbf{Three-way interactions} & \cellcolor{white}{} & \cellcolor{white}{}\\
\cmidrule{1-3}
NM x SM x TS & \cellcolor[HTML]{E4E2E2}{\textbf{Cell 6}} & \cellcolor[HTML]{C7C4C4}{\textbf{\centering{\arraybackslash{Cell 7}}}}\\
 
NM x SM x S & \cellcolor[HTML]{E4E2E2}{\textbf{Cell 8}} & \cellcolor[HTML]{C7C4C4}{\textbf{Cell 9 (\hyperref[Exp2]{Exp. 2})}}\\
 
NM x TS x S & \cellcolor{white}{\parencite[][\textsuperscript{a}]{coulombe2016}} & \cellcolor[HTML]{C7C4C4}{\textbf{Cell 10 (\hyperref[Exp3]{Exp. 3})}}\\
 
SM x TS x S & \cellcolor[HTML]{E4E2E2}{\textbf{Cell 11}} & \cellcolor[HTML]{C7C4C4}{\textbf{Cell 12}}\\*
\end{longtable}
\end{ThreePartTable}
\end{landscape}
\restoregeometry

\noindent change. Given that change over time is more likely to follow a nonlinear than a linear pattern \autocite[for a review, see][]{cudeck2007}, it could be argued that most simulation research has investigated the effect of longitudinal design factors under unrealistic conditions.

Second, all the cells corresponding to the three-way interactions with nonlinear patterns of change have not been investigated (Cells 7, 9, 10, and 12 in Table \ref{tab:systematicReviewCount}), meaning that almost no study has conducted a comprehensive investigation into measurement timing. Given that longitudinal research is needed to understand the temporal dynamics of psychological processes---as suggested by ergodic theory \autocite{molenaar2004}---it is necessary to understand how longitudinal design and analysis factors interact with each other (and with sample size) in affecting the modelling accuracy of temporal dynamics. Given that no simulation study identified in my systematic review conducted a comprehensive investigation into the effects of longitudinal design and analysis factors on modelling nonlinear change, I designed simulation studies to address these gaps.

\hypertarget{modelling-change}{%
\section{Methods of Modelling Nonlinear Patterns of Change Over Time}\label{modelling-change}}

Because my simulation experiments assumed change over time to be nonlinear, it is important to provide an overview of how nonlinear change can be modelled. On this note, I will provide an overview of two commonly employed methods for modelling nonlinear change: 1) the polynomial approach and 2) the nonlinear function approach.\footnote{It should be noted that nonlinear change can be modelled in a variety of ways, with latent change score models \parencite[e.g., ][]{orourke2021} and spline models \parencite[e.g., ][]{fine2020} offering some examples.}\textsuperscript{,}\footnote{The definition of a nonlinear function is mathematical in nature. Specifically, a nonlinear function contains at least one parameter that exists in its corresponding partial derivative (at any order). For example, in the logistic function $\uptheta + \frac{\upalpha - \uptheta}{1 + exp^(\frac{\upbeta - t}{\upgamma}}$ is nonlinear because $\upbeta$ exists in $\frac{\partial y}{\partial \upbeta}$ (in addition to $\upgamma$ existing in its corresponding partial derivative). The $n^{th}$ order polynomial function of $y = a + bx + cx^2 + ... + nx^n$ is linear because the partial derivatives with respect to any of the parameters (i.e., $1, x^2, ..., x^n$) never contain the associated parameter.} Importantly, the simulation experiments in my dissertation will use the nonlinear function approach to model nonlinear change.

Consider an example where an organization introduces a new incentive system with the goal of increasing the motivation of its employees. To assess the effectiveness of the incentive system, employees provide motivation ratings every month over a period of 360 days. Over the 360-day period, the motivation levels of the employees increase following an s-shaped pattern of change over time. One analyst decides to model the observed change using a \emph{polynomial function} shown below in Equation \ref{eq:polynomial}:
\begin{align}
  y = \mathit{a} + \mathit{b}x + \mathit{c}x^2 + \mathit{d}x^3.
  \label{eq:polynomial}
\end{align}
\noindent A second analyst decides to model the observed change using a \emph{logistic function} shown below in Equation \ref{eq:logistic1}:
\begin{align}
  y = \uptheta + \frac{\upalpha - \uptheta}{1 + e^{\frac{\upbeta -time}{\upgamma}}}
  \label{eq:logistic1}
\end{align}
\noindent  Figure \ref{fig:polynomial-vs-logistic}A shows the response pattern predicted by the polynomial function of Equation \ref{eq:polynomial} with the estimated values of each parameter (\(a\), \(b\), \(c\), and \(d\)) and Figure \ref{fig:polynomial-vs-logistic}B shows the response pattern predicted by the logistic function (Equation \ref{eq:logistic1}) along with the values estimated for each parameter (\(\uptheta\), \(\upalpha\), \(\upbeta\), and \(\upgamma\)). Although the logistic and polynomial
\begin{apaFigure}
[portrait]
[samepage]
[0cm]
{Response Patterns Predicted by Polynomial (Equation \ref{eq:polynomial}) and Logistic (Equation \ref{eq:logistic1}) Functions}
{polynomial-vs-logistic}
{0.26}
{Figures/polynomial_vs_nonlinear_plot}
{Panel A: Response pattern predicted by the polynomial function of Equation \eqref{eq:polynomial}. Panel B: Response pattern predicted by the logistic function of Equation \eqref{eq:logistic1}.}
\end{apaFigure}
\noindent functions predict nearly identical response patterns, the parameters of the logistic function have the following meaningful interpretations (see Figure \ref{fig:combined_plot}):
\begin{itemize}
\tightlist
\item
  \(\uptheta\) specifies the value at the first plateau (i.e., the starting value), and so is called the \emph{baseline} parameter (see Figure \ref{fig:combined_plot}A).
\item
  \(\upalpha\) specifies the value at the second plateau (i.e., the ending value), and so is called the the \emph{maximal elevation} parameter (see Figure \ref{fig:combined_plot}B).
\item
  \(\upbeta\) specifies the number of days required to reach half the difference between the first and second plateau (i.e., the midway point), and so is called the \emph{days-to-halfway-elevation} parameter (see Figure \ref{fig:combined_plot}C).
\item
  \(\upgamma\) specifies the number of days needed to move from the midway point to approximately 73\% of the difference between the starting and ending values (i.e., satiation point), and so is called the \emph{triquarter-halfway delta} parameter (see Figure \ref{fig:combined_plot}D).
\end{itemize}
\begin{apaFigure}
[landscape]
[samepage]
[0cm]
{Description Each Parameters Logistic Function (Equation \ref{eq:logistic1}) Functions}
{combined_plot}
{0.17}
{Figures/parameter_explanation_plot}
{Panel A: The baseline parameter ($\uptheta$) sets the starting value of the of curve, which in the current example has a value of 3.00 ($\uptheta$ = 3.00). Panel B: The maximal elevation parameter ($\upalpha$) sets the ending value of the curve, which in the current example has a value of 3.32 ($\upalpha$ = 3.32). Panel C: The days-to-halfway elevation parameter ($\upbeta$) sets the number of days needed to reach 50\% of the difference between the baseline and maximal elevation values. In the current example, the baseline-maximal elevation difference is 0.32 ($\upalpha - \uptheta$ = 3.32 - 3.00 = 0.32), and so the days-to-halfway elevation parameter defines the number of days needed to reach a value of 3.16. Given that the days-to-halfway elevation parameter is set to 180 in the current example ($\upbeta = 180.00$), then 180 days are needed to go from a value of 3.00 to a value of 3.16. Panel D: The triquarter-halfway delta parameter ($\upgamma$) sets the number of days needed to go from halfway elevation to approximately 73\% of the baseline-maximal elevation difference of 0.32 ($\upalpha - \uptheta$ = 3.32 - 3.00 = 0.32). Given that 73\% of the baseline-maximal elevation difference is 0.23 and the triquarter-halfway delta is set to 20 days ($\upgamma = 20.00$), then 20 days are needed to go from the halfway point of 3.16 to the triquarter point of approximately 3.23).}
\end{apaFigure}
\noindent Applying the parameter meanings of the logistic function to the parameter values estimated by using the logistic function (Equation \ref{eq:logistic1}), the predicted response pattern begins at a value of 3.00 (baseline) and reaches a value of 3.32 (maximal elevation) by the end of the 360-day period. The midway point of the curve is reached after 180.00 days (days-to-halfway elevation) and the satiation point is reached 20.00 days later (triquarter-halfway delta; or 200.00 days after the beginning of the incentive system is introduced). When looking at the polynomial function, it is almost impossible to meaningfully interpret the values of any of the other parameter values (aside from the `\(a\)' parameter, which indicates the starting value). Therefore, using a nonlinear function such as the logistic function provides a meaningful way to interpret nonlinear change.

\hypertarget{multilevel-and-latent-variable-approach}{%
\section{Multilevel and Latent Variable Approach}\label{multilevel-and-latent-variable-approach}}

In addition to using the logistic function to model nonlinear change, another modelling decision concerns whether to do so using the multilevel or latent growth curve framework. In my dissertation, I opted for the latent growth curve framework for two reasons. First, the latent growth curve framework allows data to be more realistically modelled than the multilevel framework. As some examples, the latent growth curve framework allows the modelling of measurement error, complex error structures, and time-varying covariates \autocite[for a review, see][]{mcneish2018}. Second, and perhaps more important, the likelihood of convergence with multilevel models decreases as the number of random-effect parameters increases due to nonpositive definitive covariance matrices \autocite[for a review, see][]{mcneish2020}. With the model I used in my simulation experiments having four random-effect parameters, it is likely that my simulation experiments would have considerable convergence issues if they use the multilevel framework. Therefore, given the convergence issues of multilevel models and the shortcoming realistically modelling data, I decided, on balance, that the strengths of the multilevel framework (e.g., more options for modelling small samples) were outweighed by its shortcomings, and decided to use a latent growth curve framework in my simulation experiments.

\hypertarget{next-steps}{%
\subsection{Next Steps}\label{next-steps}}

Given that longitudinal research is needed to understand the temporal dynamics of psychological processes, it is necessary to understand how longitudinal design and analysis factors interact with each other (and with sample size) in affecting the accuracy with which nonlinear patterns of change are modelled. With no study to my knowledge having conducted a comprehensive investigation into how longitudinal design and analysis factors affect the modelling of nonlinear change patterns, my simulation experiments are designed to address these gaps in the literature. Specifically, my simulation experiments investigate how measurement number, measurement spacing, and time structuredness affect the accuracy with which a nonlinear change pattern is modelled (see Cells 1, 5, 9, and 10 of Table \ref{tab:systematicReviewCount}/Table \ref{tab:systematicReview}).

\hypertarget{overview-of-simulation-experiments}{%
\section{Overview of Simulation Experiments}\label{overview-of-simulation-experiments}}

To investigate the effects of longitudinal design and analysis factors on modelling accuracy, I conducted three Monte Carlo experiments. Before summarizing the simulation experiments, one point needs to be mentioned regarding the maximum number of independent variables used in each experiment. No simulation experiment manipulated more than three variables because of the difficulty associated with interpreting interactions between four or more variables. Even among academics, the ability to correctly interpret interactions sharply declines when the number of independent variables increases from three to four \autocite{halford2005}. Therefore, none of my simulation experiments manipulated more than three variables so that results could be readily interpreted.

To summarize the three simulation experiments, the independent variables of each simulation experiment are listed below:
\begin{itemize}
\tightlist
\item
  Experiment 1: number of measurements, spacing of measurements, and nature of change.
\item
  Experiment 2: number of measurements, spacing of measurements, and sample size.
\item
  Experiment 3: number of measurements, sample size, and time structuredness.
\end{itemize}
\noindent The sections that follow will present each of the simulation experiments and their corresponding results.

\hypertarget{exp-1}{%
\chapter{Experiment 1}\label{exp-1}}

In Experiment 1, I investigated the number of measurements needed to obtain high model performance for the estimation of each logistic function parameter (i.e., unbiased and precise estimation) under different spacing schedules and natures of change. Before presenting the results of Experiment 1, I present my design and analysis goals. For my design goals, I conducted a 4 (measurement spacing:equal, time-interval increasing, time-interval decreasing, middle-and-extreme) x 4 (number of measurements: 5, 7, 9, 11) x 3 (nature of change: population value for the fixed-effect days-to-halfway elevation parameter {[}\(\upbeta_{fixed}\){]} of 80, 180, or 280) study. For my analysis goals, I was interested in answering two questions. First, I was interested in whether placing measurements near periods of change increases model performance. To answer my first question, I determined whether model performance under each spacing schedule increased when measurements were taken closer to periods of change.

Second, I was interested in how to space measurements when the nature of change is unknown. When the nature of change is unknown, this translates to a situation where a researcher has little to no knowledge of how change unfolds over time, and so any nature of change is a viable candidate for the true change. Therefore, to determine how to space measurements when the nature of change is unknown, I averaged the model performance of each spacing schedule across all possible nature-of-change curves and considered the spacing schedule with the highest model performance to be the best one.

\hypertarget{methods}{%
\section{Methods}\label{methods}}

\hypertarget{data-generation}{%
\subsection{Overview of Data Generation}\label{data-generation}}

\hypertarget{function-used-to-generate-each-data-set}{%
\subsubsection{Function Used to Generate Each Data Set}\label{function-used-to-generate-each-data-set}}

Data for each simulation experiment were generated using R \autocite{rstudio}. To generate the data, the \emph{multilevel logistic function} shown below in Equation \eqref{eq:logFunction-generation} was used:
\begin{align}
  y_{ij} = \uptheta_j + \frac{\upalpha_j - \uptheta_j}{{1 + e^\frac{\upbeta_j - time_i}{\upgamma_j}}} + \upepsilon_{ij}, 
\label{eq:logFunction-generation}
\end{align}
\noindent where \(\uptheta\) represents the baseline parameter, \(\upalpha\) represents the maximal elevation parameter, \(\upbeta\) represents the days-to-halfway elevation parameter, and \(\upgamma\) represents triquarter-halfway delta parameter. Note that, values for \(\uptheta\), \(\upalpha\), \(\upbeta\), and \(\upgamma\) were generated for each \emph{j} person across all \emph{i} time points, with an error value being randomly generated at each \emph{i} time point(\(\upepsilon_{ij}\); see Figure \ref{fig:combined_plot} for a review of each parameter). In other words, unique response patterns were generated for each person in each generated data set. Importantly, 1000 data sets were generated per cell.

The logistic growth function (Equation \ref{eq:logFunction-generation}) was used because it is a common pattern of organizational change {[}or institutionalization; \textcite{lawrence2001}{]}. Institutionalization curves follow an s-shaped pattern (i.e., logistic growth), and so their rates of change can be represented by the days-to-halfway elevation and triquarter-halfway delta parameters (\(\upbeta\), \(\upgamma\), respectively), and the success of the change can be defined by the magnitude of the difference between baseline and maximal elevation parameters (\(\upalpha\) - \(\uptheta\), respectively).

\hypertarget{population-values-used-for-function-parameters}{%
\subsubsection{Population Values Used for Function Parameters}\label{population-values-used-for-function-parameters}}

Table \ref{tab:parameterValues} lists the parameter values that were used for the population parameters. Given that the decisions for setting the values for the baseline, maximal elevation, and residual variance parameters were informed by past research, the discussion that follows highlights how these decisions were made. The difference between the baseline and maximal elevation parameters (\(\uptheta\) and \(\upalpha\), respectively) corresponded to the effect size most commonly observed in organizational research {[}i.e., the 50\textsuperscript{th} percentile effect size value; \textcite{bosco2015}{]}. Because the meta-analysis of \textcite{bosco2015} computed effect sizes as correlations, the 50\textsuperscript{th} percentile effect size value of \(r = .16\) was computed to a standardized effect size using the following conversion function shown in Equation \ref{eq:conversion-effect} \autocite[Chapter 7]{borenstein2009}:
\begin{align}
d = \frac{2r}{\sqrt{1 - r^2}}, 
\label{eq:conversion-effect}
\end{align}
\noindent where \(r\) is the correlation effect size. Using Equation \ref{eq:conversion-effect}, a correlation value of \(r = .16\) becomes a standardized effect size value of \(d = 0.32\). For the value of the residual variance parameter (\(\upepsilon\)), \textcite{coulombe2016} set it to the value used for the intercept variance parameter. In the current context, the intercept of the logistic function (Equation \ref{eq:logFunction-generation}) is the baseline parameter (\(\uptheta\)).\footnote{The definition of an intercept parameter is the value of a curve when no time has elapsed, and this is precisely the definition of the baseline parameter ($\uptheta$). Therefore, the variance of the intercept parameter carries the same meaning as the variance of the baseline parameter ($\uptheta_{random}$).} Given that the value for the variability of the baseline parameter was 0.05 (albeit in standard deviation units), the value used for the residual variance parameter was 0.05 (\(\upepsilon = 0.05\)). Importantly, because \textcite{coulombe2016} set covariances between parameters to zero, all the simulation experiments used zero-value covariances. Because justification for the other parameters could not be found in any of the simulation studies identified in my systematic review, values set for the other parameters were largely arbitrary.

Two last brief points need to be mentioned about how data were generated to facilitate the interpretation of the results. First, data were generated to take on units similar to that of a Likert scale (range of 1--5) by assuming a standard deviation of 1.00. Thus, previously established effect size of \(d = 0.32\) standard deviations implies an effect size of 0.32 units. Second, change was assumed to occur over a period of 360 days because many organizational processes are often governed by annual events (e.g., performance reviews, annual returns, regulations, etc.).

\hypertarget{data-modelling}{%
\subsection{Modelling of Each Generated Data Set}\label{data-modelling}}

Previously, I described how data were generated. Here, I describe how the generated data were modelled.

Each data set generated by the multilevel logistic function (Equation \ref{eq:logFunction-generation}) was analyzed using a modified latent growth curve model known as a structure latent growth curve model \autocite{preacher2015}.
\begin{ThreePartTable}
\begin{TableNotes}
\item \textit{Note. }The difference between $\upalpha$ and $\uptheta$ corresponds to the 50$\mathrm{^{th}}$ percentile Cohen's $d$ value of 0.32 in organizational psychology (Bosco et al., 2015).
\end{TableNotes}
\begin{longtable}[l]{>{\raggedright\arraybackslash}p{12 cm}c}
\caption{\label{tab:parameterValues}Values Used for Multilevel Logistic Function Parameters}\\
\toprule
Parameter Means & Value\\
\midrule
\endfirsthead
\caption[]{\label{tab:parameterValues}Values Used for Multilevel Logistic Function Parameters \textit{(continued)}}\\
\toprule
Parameter Means & Value\\
\midrule
\endhead

\endfoot
\bottomrule
\insertTableNotes
\endlastfoot
\hspace{2em}Baseline, $\uptheta$ & 3.00\\
\hspace{2em}Maximal elevation, $\upalpha$ & 3.32\\
\hspace{2em}Days-to-halfway elevation, $\upbeta$ & 180.00\\
\hspace{2em}Triquarter-halfway delta, $\upgamma$ & 20.00\\
\addlinespace\addlinespace\cmidrule{1-2}
Variability and Covariability Parameters (in Standard Deviations) & \\
\cmidrule{1-2}
\hspace{2em}Baseline standard deviation, $\uppsi_{\uptheta}$ & 0.05\\
\hspace{2em}Maximal elevation standard deviation, $\uppsi_{\upalpha}$ & 0.05\\
\hspace{2em}Days-to-halfway elevation standard deviation, $\uppsi_{\upbeta}$ & 10.00\\
\hspace{2em}Triquarter-halfway delta standard deviation, $\uppsi_{\upgamma}$ & 4.00\\
\hspace{2em}Baseline-maximal elevation covariability, $\uppsi_{\uptheta\upalpha}$ & 0.00\\
\hspace{2em}Baseline-days-to-halfway elevation covariability, $\uppsi_{\uptheta\upbeta}$ & 0.00\\
\hspace{2em}Baseline-triquarter-halfway delta covariability, $\uppsi_{\uptheta\upgamma}$ & 0.00\\
\hspace{2em}Maximal elevation-days-to-halfway elevation covariability, $\uppsi_{\upalpha\upbeta}$ & 0.00\\
\hspace{2em}Maximal elevation-triquarter-halfway delta covariability, $\uppsi_{\upalpha\upgamma}$ & 0.00\\
\hspace{2em}Days-to-halfway elevation-triquarter-halfway delta covariability, $\uppsi_{\upbeta\upgamma}$ & 0.00\\
\hspace{2em}Residual standard deviation, $\uppsi_{\upepsilon}$ & 0.05\\*
\end{longtable}
\end{ThreePartTable}
\noindent Importantly, the model fit to each generated data set estimated nine parameters: A fixed-effect parameter for each of the four logistic function parameters, a random-effect parameter for each of the four logistic function parameters, and an error parameter. As with a multilevel model, a fixed-effect parameter has a constant value across all individuals, whereas a random-effect parameter represents the variability of values across all modelled people.\footnote{Estimating a random-effect for a parameter allows person- or data-point-specific values to be computed for the parameter.} To fit the logistic function to a given data set (Equation \ref{eq:logFunction-generation}), a linear approximation of the logistic function was needed so that it could fit within the linear nature of structural equation modelling framework.\footnote{The logistic function (Equation \ref{eq:logFunction-generation}) is a nonlinear function and so cannot be directly inserted into the structural equation modelling framework because this framework only allows linear computations of matrix-matrix, matrix-vector, and vector-vector operations. Unfortunately, the algebraic operations permitted in a linear framework cannot directly reproduce the operations in the logistic function (Equation \ref{eq:logFunction-generation}) and so a linear approximation of the logistic function must be constructed so that the logistic function can be inserted into the structural equation modelling framework.} To construct a linear approximation of the logistic function, a first-order Taylor series was constructed for the logistic function. For a detailed explanation of how the logistic function was fit into the structural equation modelling framework, see Appendix \ref{structured-lgc} for an explanation of the model and Appendix \ref{structured-lgc-code} for the code used to create the model.

\hypertarget{variables-used-in-simulation-experiment}{%
\subsection{Variables Used in Simulation Experiment}\label{variables-used-in-simulation-experiment}}

\hypertarget{independent-variables}{%
\subsubsection{Independent Variables}\label{independent-variables}}

To build on current research, Experiment 1 used independent variable manipulations from a select number of previous studies. In looking at the summary of the simulation literature in Table \ref{tab:systematicReview}, the study by \textcite{coulombe2016} was the only one to investigate three longitudinal issues of interest to my dissertation, and so represented the most comprehensive investigation. Because I was also interested in investigating measurement spacing, manipulations were inspired from the only other simulation study identified by my systematic review to manipulate measurement spacing \autocite[the study by][]{timmons2015}. The sections that follow will discuss each of the variables manipulated in Experiment 1.

\hypertarget{spacing-measurements}{%
\paragraph{Spacing of Measurements}\label{spacing-measurements}}

The only simulation study identified by my systematic review that manipulated measurement spacing was \textcite{timmons2015}. Measurement spacing in \textcite{timmons2015} was manipulated in the following four ways:
\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  \emph{Equal spacing}: measurements are divided by intervals of equivalent lengths.
\item
  \emph{Time-interval increasing spacing}: intervals that divide measurements increase in length over time.
\item
  \emph{Time-interval decreasing spacing}: intervals that divide measurements decrease in length over time.
\item
  \emph{Middle-and-extreme spacing}: measurements are clustered near the beginning, middle, and end of the data collection period.
\end{enumerate}
\noindent To maintain consistency with the established literature, I manipulated measurement spacing in the same way as \textcite{timmons2015} presented above. Importantly, because \textcite{timmons2015} did not create their measurement spacing schedules with any systematicity, I developed a novel and replicable procedure for generating measurement schedules for each of the four measurement spacing conditions, which is described in Appendix \ref{measurement-schedules}. I also automated the generation of measurement schedules by creating a set of functions in R \autocite{rstudio}.

Table \ref{tab:measurementDays} lists the measurement days that were used for all measurement spacing-measurement number cells. The first column lists the type of measurement spacing (i.e., equal, time-interval increasing, time-interval decreasing, or middle-and-extreme); the second column lists the number of measurements (5, 7, 9, or 11); the third column lists the measurement days that correspond to each measurement number-measurement spacing condition; and the fourth column lists the interval lengths between the measurements. Note that the interval lengths are equal for equal spacing, increase over time for time-interval increasing spacing, and decrease over time for time-interval decreasing spacing. For cells with middle-and-extreme spacing, the measurement days and interval lengths in the middle of the measurement window have been emboldened.

\hypertarget{number-measurements}{%
\paragraph{Number of Measurements}\label{number-measurements}}

The smallest measurement number value in \textcite{coulombe2016} of three measurements could not be used in Experiment 1 (or any other simulation experiment that manipulated measurement number in my dissertation) because doing so would have created non-identified models The model used in my simulations estimated 9 parameters (\emph{p} = 9; 4 fixed-effects + 4 random-effects + 1 error)\footnote{Degrees of freedom is calculated by multiplying the number of observed variables (\textit{p}) by \textit{p} + 1 and dividing it by 2 \parencite[$\frac{p [p+1]}{2}$;][]{loehlin2017}.} and so the minimum number of measurements (or observed variables) required for model identification (and to allow model comparison) was 4. Although a measurement number of three could not be used in my manipulation of measurement number, the next highest measurement number values in \textcite{coulombe2016} of 5, 7, and 9 were used. Importantly, a larger value of 11 was added to test for a possible effect of a high measurement number. Therefore, my simulation experiments used the following values in manipulating the number of measurements: 5, 7, 9, and 11.

\newgeometry{margin=2.54cm}
\begin{landscape}\begingroup\fontsize{10}{12}\selectfont
\begin{ThreePartTable}
\begin{TableNotes}
\item \textit{Note. }For middle-and-extreme spacing levels, the measurement days and and interval lengths corresponding to the middle of measurement windows have been emboldened.
\end{TableNotes}
\begin{longtable}[l]{>{\raggedright\arraybackslash}p{4.5cm}>{\raggedright\arraybackslash}p{3cm}>{\raggedright\arraybackslash}p{6.5cm}>{\raggedright\arraybackslash}p{6cm}}
\caption{\label{tab:measurementDays}Measurement Days Used for All Measurement Number-Measurement Spacing Conditions }\\
\toprule
Spacing Schedule & Number of Measurements & Measurement Days & Interval Lengths\\
\midrule
\endfirsthead
\caption[]{\label{tab:measurementDays}Measurement Days Used for All Measurement Number-Measurement Spacing Conditions  \textit{(continued)}}\\
\toprule
Spacing Schedule & Number of Measurements & Measurement Days & Interval Lengths\\
\midrule
\endhead

\endfoot
\bottomrule
\insertTableNotes
\endlastfoot
Equal & 5 & 0, 90, 180, 270, 360 & 90, 90, 90, 90\\
 & 7 & 0, 60, 120, 180, 240, 300, 360 & 60, 60, 60, 60, 60, 60\\
 & 9 & 0, 45, 90, 135, 180, 225, 270, 315, 360 & 45, 45, 45, 45, 45, 45, 45, 45\\
 & 11 & 0, 36, 72, 108, 144, 180, 216, 252, 288, 324, 360 & 36, 36, 36, 36, 36, 36, 36, 36, 36, 36\\
\cmidrule{1-4}\addlinespace
Time-interval increasing & 5 & 0, 30, 100, 210, 360 & 30, 70, 110, 150\\
 & 7 & 0, 30, 72, 126, 192, 270, 360 & 30, 42, 54, 66, 78, 90\\
 & 9 & 0, 30, 64.29, 102.86, 145.71, 192.86, 244.29, 300, 360 & 30, 34.29, 38.57, 42.86, 47.14, 51.43, 55.71, 60\\
 & 11 & 0, 30, 61.33, 94, 128, 163.33, 200, 238, 277.33, 318, 360 & 30, 31.33, 32.67, 34, 35.33, 36.67, 38, 39.33, 40.67, 42\\
\cmidrule{1-4}\addlinespace
Time-interval decreasing & 5 & 0, 150, 260, 330, 360 & 150, 110, 70, 30\\
 & 7 & 0, 90, 168, 234, 288, 330, 360 & 90, 78, 66, 54, 42, 30\\
 & 9 & 0, 60, 115.71, 167.14, 214.29, 257.14, 295.71, 330, 360 & 60, 55.71, 51.43, 47.14, 42.86, 38.57, 34.29, 30\\
 & 11 & 0, 42, 82.67, 122, 160, 196.67, 232, 266, 298.67, 330, 360 & 42, 40.67, 39.33, 38, 36.67, 35.33, 34, 32.67, 31.33, 30\\
\cmidrule{1-4}\addlinespace
Middle-and-extreme & 5 & 1, \textbf{150, 180, 210}, 360 & 150, \textbf{30, 30}, 150\\
 & 7 & 1, 30, \textbf{150, 180, 210}, 330, 360 & 30, 120, \textbf{30, 30}, 120, 30\\
 & 9 & 1, 30, 60, \textbf{150, 180, 210}, 300, 330, 360 & 30, 30, 90, \textbf{30, 30}, 90, 30, 30\\
 & 11 & 1, 30, 60, \textbf{120, 150, 180, 210, 240,} 300, 330, 360 & 30, 30, 60, \textbf{30, 30, 30, 30}, 60, 30, 30\\*
\end{longtable}
\end{ThreePartTable}
\endgroup{}
\end{landscape}
\restoregeometry

\hypertarget{population-values-set-for-the-fixed-effect-days-to-halfway-elevation-parameter-upbeta_fixed-nature-of-change}{%
\paragraph{\texorpdfstring{Population Values Set for The Fixed-Effect Days-to-Halfway Elevation Parameter \(\upbeta_{fixed}\) (Nature of Change)}{Population Values Set for The Fixed-Effect Days-to-Halfway Elevation Parameter \textbackslash upbeta\_\{fixed\} (Nature of Change)}}\label{population-values-set-for-the-fixed-effect-days-to-halfway-elevation-parameter-upbeta_fixed-nature-of-change}}

The nature of change was manipulated by setting the days-to-halfway elevation parameter (\(\upbeta_{fixed}\)) to a value of either 80, 180, or 280 days (see Figure \ref{fig:combined_plot}A). Note that no other study in my systematic review manipulated nature of change using logistic curves and so its manipulation in Experiment 1 is, to the best of my knowledge, unique. Importantly, nature of change was manipulated to simulate situations where uncertainty exists in how change unfolds over time.

\hypertarget{constants}{%
\subsubsection{Constants}\label{constants}}

Given that each simulation experiment manipulated no more than three independent variables so that results could be readily interpreted \autocite{halford2005}, other variables had to be set to constant values. In Experiment 1, two important variables were set to constant values: sample size and time structuredness. For sample size, I set the value across all cells to the average sample size used in organizational research \autocite[\emph{n} = 225;][]{bosco2015}. For time structuredness, data across all cells were generated to be time structured (i.e., all participants provide data according to one response pattern; that is, at each time point, participants provide their data at the exact same moment).

\hypertarget{dependent-variables}{%
\subsubsection{Dependent Variables}\label{dependent-variables}}

\hypertarget{convergence}{%
\paragraph{Convergence Success Rate}\label{convergence}}

The proportion of iterations in a cell where models converged defined the \emph{convergence success rate}.\footnote{Specifically, convergence was obtained if the convergence code returned by OpenMx was 0.} Equation \eqref{eq:convergence} below shows the calculation used to compute the convergence success rate:
\begin{align}
  \text{Convergence success rate} =  \frac{\text{Number of models that successfully converged in a cell}}{n},
  \label{eq:convergence} 
\end{align}
\noindent where \emph{n} represents the total number of models run in a cell.

\hypertarget{model-performance}{%
\paragraph{Model Performance}\label{model-performance}}

Model performance was the combination of two metrics: bias and precision. More specifically, two questions were of importance in the estimation of a given logistic function parameter: 1) How well was the parameter estimated on average (bias) and 2) what was a range of values that could be expected for an estimate from the output of a single model (precision). In the two sections that follow, I will discuss each metric of model performance and the cutoffs used to determine whether estimation was unbiased and precise.

\hypertarget{bias-comp}{%
\subparagraph{Bias}\label{bias-comp}}

Bias was calculated to evaluate the accuracy with which each logistic function parameter was estimated in each experimental cell. As shown below in Equation \eqref{eq:bias}, \emph{bias} was obtained by summing the differences between the population value set for a parameter and the value estimated for the parameter by each \(i\) converged model and then dividing the sum by the number of \(N\) converged models.
\begin{align}
  \text{Bias} = \frac{\sum_i^N\text{(Population value for parameter} - \text{Average estimated value}_i)}{N}
  \label{eq:bias} 
\end{align}
\noindent Bias was calculated for the fixed- and random-effect parameters of the baseline (\(\uptheta_{fixed}\), \(\uptheta_{random}\)), maximal elevation (\(\upalpha_{fixed}\), \(\upalpha_{random}\)), days-to-halfway elevation (\(\upbeta_{fixed}\), \(\upbeta_{random}\)), and the triquarter-halfway delta parameters (\(\upgamma_{fixed}\), \(\upgamma_{random}\)) and the error parameter (\(\upepsilon\)).

\hypertarget{pres-precision}{%
\subparagraph{Precision}\label{pres-precision}}

In addition to computing bias, precision was calculated to evaluate the variability with which each parameter was estimated. Importantly, metrics used to evaluate precision in previous studies aoften ssume estimates are normally distributed (e.g., mean-squared error and empirical standard error). Because some parameters in my simulations had skewed distributions, using a metric that assumed a normal distribution would likely yield inaccurate results. Correspondingly, I used a distribution-independent definition of precision. In my simulations, \emph{precision} was defined as the range of values covered by the middle 95\% of values estimated for a logistic parameter.

\hypertarget{analysis-visualization}{%
\subsection{Analysis of Data Modelling Output and Accompanying Visualizations}\label{analysis-visualization}}

To analyse and visualize modelling performance, I calculated values for convergence success rate, bias, and precision in each experimental cell (see \protect\hyperlink{dependent-variables}{dependent variables}). The sections that follow provide details on how I analysed each dependent variable and constructed plots to visualize bias and precision.

\hypertarget{convergence-analysis}{%
\subsubsection{Analysis of Convergence Success Rate}\label{convergence-analysis}}

For the analysis of convergence success rate, the mean convergence success rate was computed for each cell in each experiment (see section on \protect\hyperlink{convergence}{convergence success rate}). Because convergence rates exhibited little variability across cells due to the nearly unanimous high rates (almost all cells across all experiments had convergence success rates above 90\%), examining the effects of any independent variable on these values would have provided little information. Therefore, I only reported the average convergence success rate for each cell (see Appendix \ref{convergence-tables}).

\hypertarget{bias-analysis}{%
\subsubsection{Analysis and Visualization of Bias}\label{bias-analysis}}

In accordance with several simulation studies, an estimate with a bias value within a \(\pm10\%\) margin of error of the parameter's population value was deemed unbiased \autocite{muthen1997}. To visualize bias, I constructed bias/precision plots. Figure \ref{fig:param-estimation-ex} shows a bias/precision plot for the fixed-effect triquarter-halfway parameter (\(\upgamma_{fixed}\)) for each measurement number and nature of change. The dots (squares, circles, triangles, diamonds) indicate the average estimated value (see \protect\hyperlink{bias-comp}{bias}). The horizontal blue line indicates the population value (\(\upgamma_{fixed}\) = 4.00) and the gray band indicates the acceptable margin of error of \(\pm10\%\) of the parameter's population value. Dots that lie within the gray margin of error are filled and dots that lie outside of the margin remain unfilled. In the current example, the average value estimated for the fixed-effect triquarter-halfway delta parameter (\(\upgamma_{fixed}\)) is only biased (i.e., lies outside the margin of error) with five measurements with a nature-of-change value of 80 (\(\upbeta_{fixed}\) = 80). Therefore, estimates are unbiased in almost all cells.

\hypertarget{precision-analysis}{%
\subsubsection{Analysis and Visualization of Precision}\label{precision-analysis}}

As discussed previously, precision was defined as the range of values covered by the middle 95\% of estimated values for a given parameter (see \protect\hyperlink{precision-mid-ext-exp1}{precision}). The cutoff value used to estimate precision directly followed from the cutoff value used for bias. Given that bias values within a \(\pm10\%\) of a parameter's population value were deemed acceptable, an acceptable value for precision should not allow any bias value above the \(\pm10\%\) cutoff. That is, the range covered by the middle 95\% of estimated values should not contain a bias value outside the \(\pm10\%\) cutoff. If the range of values covered by the middle 95\% of estimate values is conceptualized as an error bar centered on the population value, then an acceptable value for precision implies that neither
\begin{apaFigure}
[portrait]
[0cm]
{Bias/Precision Plot for the Fixed-Effect Days-to-Halfway Elevation Parameter ($\upgamma_{fixed}$)}
{param-estimation-ex}
{0.9}
{Figures/param_estimation_ex}
{Dots (squares, circles, triangles, diamonds) indicate the average estimated value and error bars show the range of values covered by the middle 95\% of the estimated values (see \nameref{pres-precision}). The horizontal blue line indicates the population value ($\upgamma_{fixed}$ = 4.00) and the gray band indicates the acceptable margin of error (i.e., $\pm$10\% of the population value) for bias. Dots that lie outside of the margin of error are unfilled and are considered biased estimates. Dots that lie inside the margin of error are filled and considered unbiased estimates. Error bars whose upper and/or lower whisker lengths exceed 10\% of the parameter's population value are light blue and indicate parameter estimation that is not precise. Error bars whose upper and/or lower whisker lengths do not excced 10\% of the parameter's population value are black and indicate parameter estimation that is precise.}
\end{apaFigure}
\noindent the lower nor upper whiskers have a length greater than 10\% of the parameter's population value. In summary, I deemed precision acceptable if no estimate within the range of values covered by the middle 95\% of estimated values had a bias value greater than 10\% of the population value, which also means that neither the lower nor upper whiskers of the error bar have a length greater than 10\% of the population value.

Like bias, I also depicted precision in bias/precision plots using error bars. Each error bar in the bias/precision plot of Figure \ref{fig:param-estimation-ex} indicates the range of values covered by the middle 95\% of estimated values in the given cell for the fixed-effect triquarter-halfway delta parameter (\(\upgamma_{fixed}\)). Importantly, if estimation is not precise, then at least one of the lower and/or upper whisker lengths exceeds 10\% of the parameter's population value. When estimation is not precise, the error bar is light blue. When estimation is precise (i.e., neither of the lower or upper whisker lengths exceed 10\% of the parameter's population value), the corresponding error bar is black. In the current example, all error bars are light blue and so precision is low in all cells.

\hypertarget{effect-size-computation-for-precision}{%
\paragraph{Effect Size Computation for Precision}\label{effect-size-computation-for-precision}}

One last statistic I calculated was an effect size value to estimate the variance in parameter estimates accounted for by each effect. Among the several effect size metrics---at a broad level, effect size metrics can represent standardized differences or variance-accounted-for measures that are corrected or uncorrected for sampling error---the corrected variance-accounted-for effect size metric of partial \(\upomega^2\) was chosen because of three desirable properties. First, partial \(\upomega^2\) provides a less biased estimate of effect size than other variance-accounted-for measures \autocite{okada2013}. Second, partial \(\upomega^2\) is more robust to assumption violations of normality and homogeneity of variance \autocite{yigit2018}. Given that parameter estimates were often non-normally distributed across cells, effect size values computed with partial \(\upomega^2\) should be relatively less biased than other variance-accounted-for effect size metrics (e.g., \(\eta^2\)). Third, partial \(\upomega^2\) provides an effect size estimate that is not diluted by the inclusion of unaccountable variance in the denominator. To compute partial \(\upomega^2\) value for each experimental effect, Equation \ref{eq:partial-omega} shown below was used:
\begin{align}
\text{partial} \upomega^2 = \frac{\sigma^2_{effect}}{\sigma^2_{effect} + MSE} 
\label{eq:partial-omega}
\end{align}
\noindent where \(\sigma^2_{effect}\) represents the variance accounted by an effect and \(MSE\) is the mean squared error. Importantly, \(\sigma^2_{effect}\) values were corrected values obtained by using the following formula in Equation \ref{eq:var-effect} for a two-way factorial design with fixed variables \autocite{howell2009}:
\begin{align}
 \sigma^2_{effect} = \frac{(a - 1)(MS_{effect} - MS_{error})}{nab},
\label{eq:var-effect}
\end{align}
\noindent where \(a\) is the number of levels in the effect, \(b\) is the number of levels in the second effect, and \(n\) is the cell size. The variance accounted by the interaction was computed using the following formula in Equation \ref{eq:var-interac}:
\begin{align}
 \sigma^2_{A x B} = \frac{(a - 1)(b-1)(MS_{AxB} - MS_{error})}{nab}. 
\label{eq:var-interac}
\end{align}
To compute partial \(\upomega^2\) values for effects, a Brown-Forsythe test was computed and the appropriate sum-of-squares terms were used to compute partial \(\upomega^2\) values. A Brown-Forsythe test was used to protect against the biasing effects of skewed distributions \autocite{brown1974}, which were observed in the parameter estimate distributions in the current simulation experiments. To compute the Brown-Forsythe test, median absolute deviations in each cell were computed by calculating the absolute difference between each \(i\) estimate and the median estimated value in the given experimental cell as shown in Equation \ref{eq:brown-forsythe} below:
\begin{align}
\text{Median absolute deviation}_i = \lvert \text{Parameter estimate}_i - \text{Median parameter estimate}_{cell} \rvert.
\label{eq:brown-forsythe}
\end{align}
\noindent An ANOVA was then computed on the median absolute deviation values (using the independent variables of the experiment and the associated interactions as predictors), with the terms in Equation \ref{eq:partial-omega} extracted from the ANOVA output to compute partial \(\upomega^2\) values.

\hypertarget{results-and-discussion}{%
\section{Results and Discussion}\label{results-and-discussion}}

In the sections that follow, I organize the results by presenting them for each spacing schedule (equal, time-interval increasing, time-interval decreasing, middle-and-extreme). The results are presented for each spacing schedule because answering my research questions first requires knowledge of these results. To answer my first question of whether model performance increases from placing measurements during periods of change, I need to determine whether model performance under each spacing schedule increases when measurements are placed near periods of change. To answer my second question of how to space measurements when the nature of change is unknown, model performance across all manipulated nature-of-change values must first be calculated for each spacing schedule. The spacing schedule that obtains the highest model performance across all nature-of-change values can then be determined as the best schedule to use.

For each spacing schedule, I will first present a concise summary table of the results and then provide a detailed report for each column of the summary table. Because the detailed reports are of considerable length, I provide concise summaries before the detailed reports to establish a framework to help interpret the detailed reports. The detailed report of each spacing schedule presents the results of each day-unit's bias/precision plot, model performance under each nature-of-change value, and then provides a qualitative summary. After providing the results for each spacing schedule, I then use the results to answer my research questions.

\hypertarget{framework-for-interpreting-results}{%
\subsection{Framework for Interpreting Results}\label{framework-for-interpreting-results}}

To conduct Experiment 1, the three variables of number of measurements (4 levels), measurement spacing (4 levels), and nature of change (3 levels) were manipulated, which yielded a total of 48 cells. Importantly, within each cell, bias and precision values were also computed for each of the nine parameters estimated by the structured latent growth curve model (for a review, see \protect\hyperlink{modelling-data-sets}{modelling of each generated data set}). Thus, because the analysis of Experiment 1 computed values for many dependent variables, interpreting the results can become overwhelming. Therefore, I will provide a framework to help the reader efficiently navigate the results section.

Because I will present the results of Experiment 1 by each level of measurement spacing, the framework I will describe in Figure \ref{fig:results-plot-primer} shows a template for the bias/precision plots that I will present for each spacing schedule. The results of each spacing schedule contain a bias/precision plot for each of the nine estimated parameters. Each bias/precision plot shows the bias and precision for the estimation of one parameter across all measurement number and nature-of change levels. Within each bias/precision plot, dots indicate the average estimated value (which indicates bias bias) and error bars represent the middle 95\% range of estimated values (which indicates precision). Bias/precision plots with black outlines show the results for day-unit parameters and plots with gray outlines show the results for Likert-unit parameters. Importantly, only the results for the day-unit parameters will be presented (i.e., fixed- and random-effect days-to-halfway elevation and triquarter-halfway delta parameters {[}\(\upbeta_{fixed}\), \(\upbeta_{random}\), \(\upgamma_{fixed}\), \(\upgamma_{random}\), respectively{]}). The results for the Likert-unit parameters (i.e., fixed- and random-effect baseline and maximal elevation parameters {[}\(\uptheta_{fixed}\), \(\uptheta_{random}\), \(\upalpha_{fixed}\), \(\upalpha_{random}\), respectively{]}) were largely trivial and so are presented in Appendix \ref{complete-versions}. Therefore, the results of each spacing schedule will only present the bias/precision plots for four parameters (i.e., the day-unit parameters).

\hypertarget{pre-processing-of-data-and-model-convergence}{%
\subsection{Pre-Processing of Data and Model Convergence}\label{pre-processing-of-data-and-model-convergence}}

After collecting the output from the simulations, non-converged models (and their corresponding parameter estimates) were removed from subsequent analyses. Table \ref{tab:conv-exp-1} in Appendix \ref{convergence-tables} provides the convergence success rates for each cell in Experiment 1. Model convergence was almost always above 90\% and convergence rates, with rates only going below 90\% in two cells (or instances) with five measurements.

\hypertarget{concise-tab}{%
\subsection{Equal Spacing}\label{concise-tab}}

For equal spacing, Table \ref{tab:summary-table-equal-spacing-exp1} provides a concise summary of the results for the day-unit parameters (see Figure \ref{fig:exp1_plot_equal} for the corresponding bias/precision plots). The sections that follow will present the results for each column of Table \ref{tab:summary-table-equal-spacing-exp1} and provide elaboration when necessary.
\begin{apaFigure}
[portrait]
[samepage]
[-0.2cm]
{Set of Bias/Precision Plots Constructed for Each Spacing Schedule in Experiment 1}
{results-plot-primer}
{.95}
{Figures/logistic_results_plot_exp1}
{A bias/precision plot is constructed for each parameter of the logistic function (see Equation \ref{eq:logFunction-generation}). Bias/precision plots with black borders show the results for day-unit parameters and plots with gray border show the results for Likert-unit parameters. For each parameter, bias and precision are shown across each combination of measurement number and nature of change.}\end{apaFigure}

Before presenting the results for equal spacing, I provide a brief description of the concise summary table created for each spacing schedule and shown for equal spacing below in Table \ref{tab:summary-table-equal-spacing-exp1}. Text within the `Highest Model Performance' column indicates the nature-of-change value that resulted in the highest model performance for each day-unit parameter. Text within the `Unbiased' and `Precise' columns indicates the number of measurements that were needed to, respectively, obtain unbiased and precise parameter estimation across all manipulated nature-of-change values. Emboldened text in the `Unbiased' and `Qualitative Description' columns indicates the measurement number that, respectively, resulted in unbiased estimation and the greatest improvements in bias and precision across all day-unit parameters and manipulated nature-of-change values. The `Error Bar Length' column indicates the average error bar length across all manipulated nature-of-change values that resulted from using the measurement number listed in the `Qualitative Description' column.

\hypertarget{nature-change-equal-exp1}{%
\subsubsection{Nature of Change That Leads to Highest Model Performance}\label{nature-change-equal-exp1}}

For equal spacing, Table \ref{tab:errorbar-equal-nc} lists the precision values (i.e., error bar lengths) for each day-unit parameter across each nature-of-change value. The `Total' column indicates the total error bar length, which is a sum of the lower (`Lower') and upper (`Upper') whisker lengths. Given that the lower and upper whisker lengths were largely equivalent for each parameter, they were largely redundant and so were not reported for equal spacing. Although model performance was determined by bias and precision, results for bias were not reported because the differences in bias across the nature-of-change values were negligible.

\newgeometry{margin=2.54cm}
\begin{landscape}
\begin{ThreePartTable}
\begin{TableNotes}
\item \textit{Note. }`Highest Model Performance' indicates the curve that resulted in the highest model performance (largely determined by precision; see \hyperref[nature-change-equal-exp1]{nature of change}). Emboldened text in the `Unbiased' and `Qualitative Description' columns indicates the number of measurements that, respectively, resulted in unbiased estimation and the greatest improvements in bias and precision across all day-unit parameters (note that acceptable precision was not obtained in the estimation of all day-unit parameters with equal spacing). `Error Bar Length' indicates the average error bar length value across all nature-of-change values that resulted from using the measurement number in the `Qualitative Description' column. Parameter names and population values are as follows: $\upbeta_{fixed}$ = fixed-effect days-to-halfway elevation parameter = {80, 180, 280}; $\upgamma_{fixed}$ = fixed-effect triquarter-halfway delta parameter = 20; $\upbeta_{random}$ = random-effect days-to-halfway elevation parameter = 10; $\upgamma_{random}$ = random-effect triquarter-halfway delta parameter = 4. NM = number of measurements.
\end{TableNotes}
\begin{longtable}[l]{>{\raggedright\arraybackslash}p{2cm}>{\centering\arraybackslash}p{5cm}>{\centering\arraybackslash}p{2.5cm}>{\centering\arraybackslash}p{3cm}>{\raggedright\arraybackslash}p{6.5cm}>{\centering\arraybackslash}p{3cm}}
\caption{\label{tab:summary-table-equal-spacing-exp1}Concise Summary of Results for Equal Spacing in Experiment 1}\\
\toprule
\multicolumn{4}{c}{ } & \multicolumn{2}{c}{Summary} \\
\cmidrule(l{3pt}r{3pt}){5-6}
Parameter & Highest Model Performance & Unbiased & Precise & Qualitative Description & Error Bar Length\\
\midrule
\thead[lt]{$\upbeta_{fixed}$ \\ (Figure \ref{fig:exp1_plot_equal}A)} & $\upbeta_{fixed}$ = 180 & All cells & All cells & Largest improvements in precision with \textbf{NM = 7} & 5.64\\
\cmidrule{1-6}
\thead[lt]{$\gamma_{fixed}$ \\ (Figure \ref{fig:exp1_plot_equal}B)} & $\upbeta_{fixed}$ = 180 & All cells & No cells & Largest improvements in precision with \textbf{NM = 7} & 4.37\\
\cmidrule{1-6}
\thead[lt]{$\upbeta_{random}$ \\ (Figure \ref{fig:exp1_plot_equal}C)} & $\upbeta_{fixed}$ = 180 & All cells & No cells & Largest improvements in precision with \textbf{NM = 7} & 7.74\\
\cmidrule{1-6}
\thead[lt]{$\upgamma_{random}$ \\ (Figure \ref{fig:exp1_plot_equal}D)} & $\upbeta_{fixed}$ = 180 & \textbf{NM $\boldsymbol{\ge}$ 9} & No cells & Largest improvements in bias and precision with \textbf{NM = 7} & 7.02\\
\bottomrule
\insertTableNotes
\end{longtable}
\end{ThreePartTable}
\end{landscape}
\restoregeometry

Note that error bar lengths were obtained by computing the average length across all manipulated number of measurements. The columns shaded in gray indicate the nature-of-change value that resulted in the highest precision (i.e., shortest error bar lengths) for equal spacing. For equal spacing, precision was highest (i.e., shortest error bar lengths) with a nature-of-change value of 180 for all day-unit parameters with one exception (i.e., see the `Highest Model Performance' column in Table \ref{tab:summary-table-equal-spacing-exp1}). Importantly, with a nature-of-change value of 180, measurements were taken closer to periods of change under equal spacing than with other nature-of-change values (see Figure \ref{fig:equal-spacing-nc}). Therefore, it appears that placing measurements closer to periods of change increased model performance with equal spacing.
\begin{ThreePartTable}
\begin{TableNotes}
\item \textit{Note. }`Total' indicates the total error bar length, which is a sum of the lower (`Lower') and upper (`Upper') whisker lengths. Parameter names and population values are as follows: $\upbeta_{fixed}$ = fixed-effect days-to-halfway elevation parameter = {80, 180, 280}; $\upgamma_{fixed}$ = fixed-effect triquarter-halfway delta parameter = 20; $\upbeta_{random}$ = random-effect days-to-halfway elevation parameter = 10; $\upgamma_{random}$ = random-effect triquarter-halfway delta parameter = 4. Note that error bar lengths were calculated by computing the average error bar length value across all number-of-measurement (NM) values (NM $\in$ \{5, 7, 9, 11\}). Columns shaded in gray indicate the nature-of-change value that results in the shortest error bar and whisker lengths.
\item[a] Error bar length is longest in this case because of the existence of high-value outliers (see Figure \ref{fig:density_gamma_equal}).
\end{TableNotes}
\begin{longtable}[l]{>{\raggedright\arraybackslash}p{3cm}ccc>{}c>{}c>{}cccc}
\caption{\label{tab:errorbar-equal-nc}Error Bar Lengths Across Nature-of-Change Values Under Equal Spacing in Experiment 1}\\
\toprule
\multicolumn{1}{c}{ } & \multicolumn{9}{c}{Population Value of $\upbeta_{fixed}$} \\
\cmidrule(l{3pt}r{3pt}){2-10}
\multicolumn{1}{c}{ } & \multicolumn{3}{c}{80} & \multicolumn{3}{c}{180} & \multicolumn{3}{c}{280} \\
\cmidrule(l{3pt}r{3pt}){2-4} \cmidrule(l{3pt}r{3pt}){5-7} \cmidrule(l{3pt}r{3pt}){8-10}
Parameter & Lower & Upper & Total & Lower & Upper & Total & Lower & Upper & Total\\
\midrule
\thead[lt]{$\upbeta_{fixed}$ \\ (Figure \ref{fig:exp1_plot_equal}A)} & 4.42 & 4.12 & 8.54 & \cellcolor[HTML]{DFDEDE}{2.46} & \cellcolor[HTML]{DFDEDE}{2.32} & \cellcolor[HTML]{DFDEDE}{4.78} & 4.09 & 4.16 & 8.25\\
\thead[lt]{$\upgamma_{fixed}$ \\ (Figure \ref{fig:exp1_plot_equal}B)} & 4.84 & 4.69 & 9.53 & \cellcolor[HTML]{DFDEDE}{4.95} & \cellcolor[HTML]{DFDEDE}{3.7} & \cellcolor[HTML]{DFDEDE}{8.65} & 4.79 & 4.65 & 9.44\\
\thead[lt]{$\upbeta_{random}$ \\ (Figure \ref{fig:exp1_plot_equal}C)} & 4.74 & 3.88 & 8.62 & \cellcolor[HTML]{DFDEDE}{3.96} & \cellcolor[HTML]{DFDEDE}{3.55} & \cellcolor[HTML]{DFDEDE}{7.51} & 4.77 & 4.05 & 8.82\\
\thead[lt]{$\upgamma_{random}$ \\ (Figure \ref{fig:exp1_plot_equal}D)} & 3.00 & 5.52 & 8.52 & \cellcolor[HTML]{DFDEDE}{3.00} & \cellcolor[HTML]{DFDEDE}{13.05\textsuperscript{a}} & \cellcolor[HTML]{DFDEDE}{16.05} & 3.00 & 5.78 & 8.78\\
\bottomrule
\insertTableNotes
\end{longtable}
\end{ThreePartTable}
\begin{apaFigure}
[portrait]
[samepage]
[0cm]
{Model Performance Status Across Nature-of-Change Values With Equal Spacing}
{equal-spacing-nc}
{0.25}
{Figures/midpoint_180_plot}
{Model performance was highest when measurements were taken closer to periods of greater change, which resulted with a nature-of-change value of 180 with equal spacing. Text prints error bar lengths that resulted when model performance was highest (see Table \ref{tab:errorbar-equal-nc}).}
\end{apaFigure}
To understand why precision for the random-effect triquarter-halfway elevation parameter (\(\upgamma_{random}\)) was lower with a nature-of-change value of 180, I looked at the

\noindent distribution of estimated values. Figure \ref{fig:density_gamma_equal} shows the distribution of values (i.e., density plots) estimated for the random-effect triquarter-halfway elevation parameter (\(\upgamma_{random}\)) for each nature-of-change level with five measurements. Importantly, the error bars in the bias/precision plot of Figure \ref{fig:exp1_plot_equal}D with five measurements are created from the density plots shown in Figure \ref{fig:density_gamma_equal}. Panel A shows the density plot with a nature-of-change value of 80 (\(\upbeta_{fixed}\) = 80). Panel B shows the density plot with a with a nature-of-change value of 180 (\(\upbeta_{fixed}\) = 180). Panel C shows the density plot with a with a nature-of-change value of 280 (\(\upbeta_{fixed}\) = 280). Regions shaded in gray represent the middle 95\% of estimated values and the width of the shaded regions is indicated by the length of the horizontal error bars. As originally confirmed by Table \ref{tab:errorbar-equal-nc}, Figure \ref{fig:density_gamma_equal}B shows that precision was indeed lowest (i.e., longer error bars) with a nature of change of 180. In looking across the density plots in Figure \ref{fig:density_gamma_equal}, precision was lowest (i.e., longest error bars) for the random-effect triquarter-halfway parameter (\(\upgamma_{random}\)) with a nature-of-change value of 180 because of the existence of high-value outliers.

In summary, under equal spacing, model performance for all the day-unit parameters was greatest when the nature-of-change value set by the fixed-effect days-to-halfway elevation parameter (\(\upbeta_{fixed}\)) had a value of 180. The one exception to this result was that model performance (as indicated by precision) was lower for the random-effect triquarter-halfway elevation parameter (\(\upgamma_{random}\)) with a nature-of-change value of 180 because of high-value outliers.

\hypertarget{bias-equal-exp1}{%
\subsubsection{Bias}\label{bias-equal-exp1}}

Before presenting the results for bias, I provide a description of the set of bias/precision plots shown in Figure \ref{fig:exp1_plot_equal} and in the results sections for the other spacing schedules in Experiment 1. Figure \ref{fig:exp1_plot_equal} shows the bias/precision plots for each day-unit parameter and Table \ref{tab:omega-exp1-equal} provides the partial \(\upomega^2\) values for each independent variable of each day-unit parameter. In Figure \ref{fig:exp1_plot_equal}, blue horizontal lines indicate the population values for each parameter (with population values of \(\upbeta_{fixed} \in\) \{80, 180, 280\}, \(\upbeta_{random}\) = 10.00, \(\upgamma_{fixed}\) = 20.00, and \(\upgamma_{random}\) = 4.00). Gray bands indicate the \(\pm 10\%\) margin of error for each parameter and unfilled dots indicate cells with average parameter estimates outside of the margin. Error bars represent the middle 95\% of estimated values, with light blue error bars indicating imprecise estimation. I considered dots that fell outside the gray bands as biased and error bar lengths with at least one whisker length exceeding the 10\% cutoff (i.e., or longer than the portion of the gray band underlying the whisker) as imprecise. Panels A--B show the bias/precision plots for the fixed- and random-effect days-to-halfway elevation parameters (\(\upbeta_{fixed}\) and \(\upbeta_{random}\), respectively). Panels C--D show the bias/precision plots for the fixed- and random-effect triquarter-halfway delta parameters (\(\upgamma_{fixed}\) and \(\upgamma_{random}\), respectively). Note that random-effect parameter units are in standard deviation units. Importantly, across all population values used for the fixed-effect days-to-halfway elevation parameter (\(\upbeta_{fixed}\)), the acceptable amount of bias and precision was based on a population value of 180.
\begin{apaFigure}
[portrait]
[samepage]
[0cm]
{Density Plots of the Random-Effect Triquarter-Halfway Delta ($\upgamma_{random}$; Figure \ref{fig:exp1_plot_equal}D) With Equal Spacing in Experiment 1 (95\% Error Bars)}
{density_gamma_equal}
{0.16}
{Figures/density_plots_equal_gamma_exp1}
{Regions shaded in in gray represent the middle 95\% of estimated values and the width of the shaded regions is indicated by the length of the horizontal error bars. The error bar length if longest when the nature-of-change value is 180. $\upgamma_{random}$ = random-effect triquarter-halfway delta parameter, with population value of 4.00, NM = number of measurements.}
\end{apaFigure}
With respect to bias for equal spacing, estimates were biased (i.e., above the acceptable 10\% cutoff) for each day-unit parameter in the following cells:
\begin{itemize}
\tightlist
\item
  fixed-effect days-to-halfway elevation parameter (\(\upbeta_{fixed}\); Figure \ref{fig:exp1_plot_equal}A): no cells.
\item
  fixed-effect triquarter-halfway delta parameter (\(\upgamma_{fixed}\); Figure \ref{fig:exp1_plot_equal}B): no cells.
\item
  random-effect days-to-halfway elevation parameter (\(\upbeta_{random}\); Figure \ref{fig:exp1_plot_equal}C): no cells.
\item
  random-effect triquarter-halfway elevation parameter (\(\upgamma_{random}\); Figure \ref{fig:exp1_plot_equal}D): five measurements with all manipulated nature-of-change values and seven measurements with nature-of-change values of 180 and 280.
\end{itemize}
In summary, with equal spacing, estimation of all the day-unit parameters across all manipulated nature-of-change values was unbiased using nine or more measurements, which is indicated by the emboldened text in the `Unbiased' column of Table \ref{tab:summary-table-equal-spacing-exp1}.

\hypertarget{precision-equal-exp1}{%
\subsubsection{Precision}\label{precision-equal-exp1}}

With respect to precision for equal spacing, estimates were imprecise (i.e., error bar length with at least one whisker length exceeding 10\% of a parameter's population value) in the following cells for each day-unit parameter:
\begin{itemize}
\tightlist
\item
  fixed-effect days-to-halfway elevation parameter (\(\upbeta_{fixed}\); Figure \ref{fig:exp1_plot_equal}A): no cells.
\item
  fixed-effect triquarter-halfway delta parameter (\(\upgamma_{fixed}\); Figure \ref{fig:exp1_plot_equal}B): all cells.
\item
  random-effect days-to-halfway elevation parameter (\(\upbeta_{random}\); Figure \ref{fig:exp1_plot_equal}C): all cells.
\item
  random-effect triquarter-halfway delta parameter {[}\(\upgamma_{random}\){]} in Figure \ref{fig:exp1_plot_equal}D): all cells.
\end{itemize}
In summary, with equal spacing, estimation across all manipulated nature-of-change values was only precise for the fixed-effect days-to-halfway elevation parameter (\(\upbeta_{fixed}\)) with five or more measurements. No manipulated measurement number resulted in precise estimation of the fixed-effect triquarter-halfway delta parameter (\(\upgamma_{fixed}\)) or the random-effect day-unit parameters (see the `Precise' column of Table \ref{tab:summary-table-equal-spacing-exp1}).
\begin{apaFigure}
[portrait]
[samepage]
[0cm]
{Bias/Precision Plots for Day-Unit Parameters With Equal Spacing in Experiment 1}
{exp1_plot_equal}
{0.16}
{Figures/exp1_plot_days_equal spacing}
{Panel A: Bias/precision plot for the fixed-effect days-to-halfway elevation parameter ($\upbeta_{fixed}$). Panel B: Bias/precision plot for the fixed-effect triquarter-halfway elevation parameter ($\upgamma_{fixed}$). Panel C: Bias/precision plot for the random-effect days-to-halfway elevation parameter ($\upbeta_{random}$). Panel D: Bias/precision plot for the random-effect triquarter-halfway elevation parameter ($\upgamma_{random}$). Blue horizontal lines in each panel represent the population value for each parameter. Population values for each day-unit parameter are as follows: $\upbeta_{fixed} \in$ {80, 180, 280}, $\upbeta_{random}$ = 10.00, $\upgamma_{fixed}$ = 20.00, $\upgamma_{random}$ = 4.00. Gray bands indicate the $\pm 10\%$ margin of error for each parameter and unfilled dots indicate cells with average parameter estimates outside of the margin or biased estimates. Error bars represent the middle 95\% of estimated values, with light blue error bars indicating imprecise estimation. I considered dots that fell outside the gray bands as biased and error bar lengths with at least one whisker length exceeding the 10\% cutoff (i.e., or longer than the portion of the gray band underlying the whisker) as imprecise. Note that random-effect parameter units are in standard deviation units. Importantly, across all nature-of-change values (i.e., population values used for $\upbeta_{fixed}$), the acceptable amount of bias and precision was based on a population value of 180. See Table \ref{tab:param-exp-1} for specific values estimated for each parameter and Table \ref{tab:omega-exp1-equal} for $\upomega^2$ effect size values.}
\end{apaFigure}
\begin{ThreePartTable}
\begin{TableNotes}
\item \textit{Note. }NM = number of measurements $\in$ \{5, 7, 9, 11\}, NC = nature of change (population value set for $\upbeta_{fixed}$ $\in$ \{80, 180, 280\}), NM x NC = interaction between number of measurements and population value set for $\upbeta_{fixed}$. $\upbeta_{fixed}$ = fixed-effect days-to-halfway elevation parameter,
           $\upgamma_{fixed}$ = fixed-effect triquarter-halfway delta parameter, 
           $\upbeta_{random}$ = random-effect days-to-halfway elevation parameter, and 
           $\upgamma_{random}$ = random-effect triquarter-halfway delta parameter.
\end{TableNotes}
\begin{longtable}[l]{>{\raggedright\arraybackslash}p{6cm}ccc}
\caption{\label{tab:omega-exp1-equal}Partial $\upomega^2$ Values for Manipulated Variables With Equal Spacing in Experiment 1}\\
\toprule
\multicolumn{1}{c}{ } & \multicolumn{3}{c}{Effect} \\
\cmidrule(l{3pt}r{3pt}){2-4}
Parameter & NM & NC & NM x NC\\
\midrule
$\upbeta_{fixed}$ (Figure \ref{fig:exp1_plot_equal}A) & 0.02 & 0.00 & 0.01\\
$\upbeta_{random}$ (Figure \ref{fig:exp1_plot_equal}B) & 0.29 & 0.02 & 0.02\\
$\upgamma_{fixed}$ (Figure \ref{fig:exp1_plot_equal}C) & 0.36 & 0.01 & 0.03\\
$\upgamma_{random}$ (Figure \ref{fig:exp1_plot_equal}D) & 0.21 & 0.03 & 0.04\\
\bottomrule
\insertTableNotes
\end{longtable}
\end{ThreePartTable}
\hypertarget{qualitative-equal-exp1}{%
\subsubsection{Qualitative Description}\label{qualitative-equal-exp1}}

Although no manipulated measurement number resulted in precise estimation of all the day-unit parameters, the largest improvements in precision (and bias) resulted from using moderate measurements numbers. With respect to bias under equal spacing, the largest improvements in bias across all manipulated nature-of-change values resulted from using the following measurement numbers for the following day-unit parameters (note that only the random-effect triquarter halfway delta parameter {[}\(\upgamma_{random}\){]} had instances of high bias):
\begin{itemize}
\tightlist
\item
  random-effect triquarter-halfway delta parameters (\(\upgamma_{random}\)): seven measurements.
\end{itemize}
\noindent With respect to precision under equal spacing, the largest improvements precision in the estimation of all day-unit parameters (except the fixed-effect days-to-halfway elevation parameter {[}\(\upbeta_{fixed}\){]}) were obtained with following measurement numbers:
\begin{itemize}
\tightlist
\item
  fixed-effect triquarter-halfway delta parameter (\(\upgamma_{fixed}\)): seven measurements, which resulted in a maximum error bar length of 4.37 days.
\item
  random-effect days-to-halfway elevation parameter (\(\upbeta_{random}\)): seven measurements, which resulted in a maximum error bar length of 7.74 days.
\item
  random-effect triquarter-halfway delta parameter (\(\upgamma_{random}\)): seven measurements, which resulted in a maximum error bar length of 7.02 days.
\end{itemize}
\noindent Therefore, for equal spacing, seven measurements led to the greatest improvements in bias and precision in the estimation of all day-unit parameters across all manipulated nature-of-change values (see the emboldened text in the `Qualitative Description' column of Table \ref{tab:summary-table-equal-spacing-exp1}).

\hypertarget{summary-of-results-with-equal-spacing}{%
\subsubsection{Summary of Results With Equal Spacing}\label{summary-of-results-with-equal-spacing}}

In summarizing the results for equal spacing, model performance was highest across all day-unit parameters (with the random-effect days-to-halfway elevation parameter (\(\upgamma_{random}\)) being an exception) when measurements were placed closer to periods of change, which occurred with a nature-of-change value of 180 (see \protect\hyperlink{nature-change-equal-exp1}{highest model performance}). Unbiased estimation of all the day-unit parameters across all manipulated nature-of-change values resulted from using nine or more measurements (see \protect\hyperlink{bias-equal-exp1}{bias}). Precise estimation of all the day-unit parameters was never obtained with any manipulated measurement number (see \protect\hyperlink{precision-equal-exp1}{precision}). Although it may be discouraging that no manipulated measurement number under equal spacing resulted in precise estimation of all the day-unit parameters, the largest improvements in precision (and bias) across all day-unit parameters were obtained with moderate measurement numbers. With equal spacing, the largest improvements in bias and precision in the estimation of all day-unit parameters across all manipulated nature-of-change values were obtained using seven measurements (see \protect\hyperlink{qualitative-equal-exp1}{Qualitative Description}).

\hypertarget{time-interval-increasing-spacing}{%
\subsection{Time-Interval Increasing Spacing}\label{time-interval-increasing-spacing}}

For time-interval increasing spacing, Table \ref{tab:summary-table-time-inc-exp1} provides a concise summary of the results for the day-unit parameters (see Figure \ref{fig:exp1_plot_time_inc} for the corresponding bias/precision plots). The sections that follow will present the results for each column of Table \ref{tab:summary-table-time-inc-exp1} and provide elaboration when necessary (for a description of Table \ref{tab:summary-table-time-inc-exp1}, see \protect\hyperlink{concise-tab}{concise summary table}).

\hypertarget{nature-change-time-inc-exp1}{%
\subsubsection{Nature of Change That Leads to Highest Model Performance}\label{nature-change-time-inc-exp1}}

For time-interval increasing spacing, Table \ref{tab:errorbar-time-inc-nc} lists the precision values (i.e., error bar lengths) for each day-unit parameter across each nature-of-change value. The `Total' column indicates the total error bar length, which is a sum of the the lower (`Lower') and upper (`Upper') whisker lengths. Given that the lower and upper whisker lengths were largely equivalent for each parameter, they were largely redundant and so were not reported for the remainder of the results for time-interval increasing spacing. Although model performance was determined by bias and precision, results for bias were not reported because the differences in bias across the nature-of-change values were negligible. Note that error bar lengths were obtained by computing the average length across all manipulated number of measurements. The columns shaded in gray indicate the nature of change where precision is highest (i.e., shortest error bar lengths). For time-interval increasing spacing, precision was lowest (i.e., longest error bars) with a nature-of-change value of 80 for all day-unit parameters (see the `Highest Model Performance' in Table \ref{tab:summary-table-time-inc-exp1}). Importantly, with a nature-of-change value of 80, measurements were taken closer to periods of change under time-interval increasing spacing than with other nature-of-change values (see Figure \ref{fig:time-inc-spacing-nc}). Therefore, it appears that placing measurements closer to periods of change increased model performance with time-interval increasing spacing.
\begin{apaFigure}
[portrait]
[samepage]
[0cm]
{Model Performance Status Across Nature-of-Change Values With Time-Interval Increasing Spacing}
{time-inc-spacing-nc}
{0.25}
{Figures/midpoint_80_plot}
{Model performance was highest when measurements were taken closer to periods of greater change, which resulted with a nature-of-change value of 80 with equal spacing. Text prints error bar lengths that resulted when model performance was highest (see Table \ref{tab:errorbar-time-inc-nc}).}
\end{apaFigure}
\hypertarget{bias-time-inc-exp1}{%
\subsubsection{Bias}\label{bias-time-inc-exp1}}

With respect to bias for time-interval increasing spacing, estimates were biased (i.e., above the acceptable 10\% cutoff) for each day-unit parameter in the following cells:
\begin{itemize}
\tightlist
\item
  fixed-effect days-to-halfway elevation parameter (\(\upbeta_{fixed}\); Figure \ref{fig:exp1_plot_time_inc}A): no cells.
\item
  fixed-effect triquarter-halfway delta parameter (\(\upgamma_{fixed}\); Figure \ref{fig:exp1_plot_time_inc}B): no cells
\item
  random-effect days-to-halfway elevation parameter (\(\upbeta_{random}\); Figure \ref{fig:exp1_plot_time_inc}C): five measurements with a nature-of-change value of 280.
\item
  random-effect days-to-halfway elevation parameter (\(\upbeta_{random}\); Figure \ref{fig:exp1_plot_time_inc}C): five measurements with all nature-of-change values and seven measurements with nature-of-change values of 180 and 280.
\end{itemize}
In summary, with time-interval increasing spacing, estimation of all the day-unit parameters across all manipulated nature-of-change values was unbiased using nine or more measurements, which is indicated by the emboldened text in the `Unbiased' column of Table \ref{tab:summary-table-time-inc-exp1}.

\hypertarget{precision-time-inc-exp1}{%
\subsubsection{Precision}\label{precision-time-inc-exp1}}

With respect to precision for time-interval increasing spacing, estimates were imprecise (i.e., error bar length with at least one whisker length exceeding 10\% of a parameter's population value) in the following cells for each day-unit parameter:



\end{document}
